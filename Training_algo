name: Model Training Component
description: Universal training component supporting ALL major clustering algorithms with automatic parameter defaults and auto-tuning capabilities.

inputs:
  - name: train_data
    type: Data
    description: 'Training data from preprocessing component'
  - name: algorithm
    type: String
    description: 'Clustering algorithm name (KMeans, DBSCAN, HDBSCAN, etc.)'
    default: 'KMeans'
  - name: parameters
    type: String
    description: 'Algorithm parameters as JSON string (optional, uses defaults if empty)'
    default: '{}'
  - name: auto_tune
    type: String
    description: 'Enable auto-tuning of parameters (true/false)'
    default: 'false'
  - name: training_mode
    type: String
    description: 'Training mode: auto, fit, fit_predict, function_call'
    default: 'auto'

outputs:
  - name: model
    type: Model
    description: 'Trained clustering model'
  - name: labels
    type: Data
    description: 'Training cluster labels'
  - name: metadata
    type: Data
    description: 'Training metadata and results'

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        pip install pandas==2.0.3 numpy==1.24.3 scikit-learn==1.3.0 scipy==1.11.1 && exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import pickle
        import argparse
        import warnings
        import numpy as np
        import pandas as pd
        import glob
        from pathlib import Path
        
        warnings.filterwarnings('ignore')
        
        # Algorithm Registry
        ALGORITHM_REGISTRY = {
            'KMeans': {
                'module': 'sklearn.cluster',
                'class': 'KMeans',
                'category': 'centroid',
                'has_fit': True,
                'has_fit_predict': True,
                'has_predict': True,
                'has_labels': True,
                'preferred_training': 'fit',
                'preferred_inference': 'native',
                'default_params': {
                    'n_clusters': 8,
                    'init': 'k-means++',
                    'n_init': 10,
                    'max_iter': 300,
                    'tol': 0.0001,
                    'random_state': 42,
                    'algorithm': 'lloyd'
                }
            },
            'MiniBatchKMeans': {
                'module': 'sklearn.cluster',
                'class': 'MiniBatchKMeans',
                'category': 'centroid',
                'has_fit': True,
                'has_fit_predict': True,
                'has_predict': True,
                'has_labels': True,
                'preferred_training': 'fit',
                'preferred_inference': 'native',
                'default_params': {
                    'n_clusters': 8,
                    'init': 'k-means++',
                    'max_iter': 100,
                    'batch_size': 256,
                    'random_state': 42,
                    'n_init': 3
                }
            },
            'BisectingKMeans': {
                'module': 'sklearn.cluster',
                'class': 'BisectingKMeans',
                'category': 'centroid',
                'has_fit': True,
                'has_fit_predict': True,
                'has_predict': True,
                'has_labels': True,
                'preferred_training': 'fit',
                'preferred_inference': 'native',
                'default_params': {
                    'n_clusters': 8,
                    'init': 'k-means++',
                    'n_init': 1,
                    'max_iter': 300,
                    'random_state': 42,
                    'bisecting_strategy': 'biggest_inertia'
                }
            },
            'DBSCAN': {
                'module': 'sklearn.cluster',
                'class': 'DBSCAN',
                'category': 'density',
                'has_fit': True,
                'has_fit_predict': True,
                'has_predict': False,
                'has_labels': True,
                'allows_noise': True,
                'preferred_training': 'fit_predict',
                'preferred_inference': 'nearest_neighbor',
                'default_params': {
                    'eps': 0.5,
                    'min_samples': 5,
                    'metric': 'euclidean',
                    'algorithm': 'auto',
                    'leaf_size': 30
                }
            },
            'OPTICS': {
                'module': 'sklearn.cluster',
                'class': 'OPTICS',
                'category': 'density',
                'has_fit': True,
                'has_fit_predict': True,
                'has_predict': False,
                'has_labels': True,
                'allows_noise': True,
                'preferred_training': 'fit_predict',
                'preferred_inference': 'nearest_neighbor',
                'default_params': {
                    'min_samples': 5,
                    'metric': 'euclidean',
                    'cluster_method': 'xi',
                    'xi': 0.05,
                    'algorithm': 'auto'
                }
            },
            'AgglomerativeClustering': {
                'module': 'sklearn.cluster',
                'class': 'AgglomerativeClustering',
                'category': 'hierarchical',
                'has_fit': True,
                'has_fit_predict': True,
                'has_predict': False,
                'has_labels': True,
                'preferred_training': 'fit_predict',
                'preferred_inference': 'nearest_neighbor',
                'default_params': {
                    'n_clusters': 2,
                    'linkage': 'ward',
                    'affinity': 'euclidean'
                }
            },
            'BIRCH': {
                'module': 'sklearn.cluster',
                'class': 'BIRCH',
                'category': 'hierarchical',
                'has_fit': True,
                'has_fit_predict': True,
                'has_predict': True,
                'has_labels': True,
                'preferred_training': 'fit',
                'preferred_inference': 'native',
                'default_params': {
                    'n_clusters': 3,
                    'threshold': 0.5,
                    'branching_factor': 50
                }
            },
            'GaussianMixture': {
                'module': 'sklearn.mixture',
                'class': 'GaussianMixture',
                'category': 'distribution',
                'has_fit': True,
                'has_fit_predict': False,
                'has_predict': True,
                'has_labels': False,
                'has_predict_proba': True,
                'preferred_training': 'fit',
                'preferred_inference': 'native',
                'default_params': {
                    'n_components': 3,
                    'covariance_type': 'full',
                    'tol': 0.001,
                    'max_iter': 100,
                    'n_init': 1,
                    'init_params': 'kmeans',
                    'random_state': 42
                }
            },
            'BayesianGaussianMixture': {
                'module': 'sklearn.mixture',
                'class': 'BayesianGaussianMixture',
                'category': 'distribution',
                'has_fit': True,
                'has_fit_predict': False,
                'has_predict': True,
                'has_labels': False,
                'has_predict_proba': True,
                'preferred_training': 'fit',
                'preferred_inference': 'native',
                'default_params': {
                    'n_components': 3,
                    'covariance_type': 'full',
                    'tol': 0.001,
                    'max_iter': 100,
                    'n_init': 1,
                    'init_params': 'kmeans',
                    'random_state': 42
                }
            },
            'MeanShift': {
                'module': 'sklearn.cluster',
                'class': 'MeanShift',
                'category': 'other',
                'has_fit': True,
                'has_fit_predict': True,
                'has_predict': True,
                'has_labels': True,
                'preferred_training': 'fit',
                'preferred_inference': 'native',
                'default_params': {
                    'bandwidth': None,
                    'bin_seeding': False,
                    'min_bin_freq': 1,
                    'cluster_all': True
                }
            },
            'AffinityPropagation': {
                'module': 'sklearn.cluster',
                'class': 'AffinityPropagation',
                'category': 'other',
                'has_fit': True,
                'has_fit_predict': True,
                'has_predict': True,
                'has_labels': True,
                'preferred_training': 'fit',
                'preferred_inference': 'native',
                'default_params': {
                    'damping': 0.5,
                    'max_iter': 200,
                    'convergence_iter': 15,
                    'random_state': 42
                }
            },
            'SpectralClustering': {
                'module': 'sklearn.cluster',
                'class': 'SpectralClustering',
                'category': 'other',
                'has_fit': False,
                'has_fit_predict': True,
                'has_predict': False,
                'has_labels': True,
                'preferred_training': 'fit_predict',
                'preferred_inference': 'nearest_neighbor',
                'default_params': {
                    'n_clusters': 8,
                    'random_state': 42,
                    'n_init': 10,
                    'gamma': 1.0,
                    'affinity': 'rbf',
                    'n_neighbors': 10,
                    'assign_labels': 'kmeans'
                }
            }
        }
        
        def find_data_file(input_path):
            #Find the actual data file in the input path - robust version#
            print(f"Searching for data file in: {input_path}")
            
            # Case 1: Direct file path
            if os.path.isfile(input_path):
                print(f"Input is a file: {input_path}")
                return input_path
            
            # Case 2: Directory - search for data files
            if os.path.isdir(input_path):
                print(f"Input is a directory, searching for data file...")
                
                # Try common patterns in order of preference
                patterns = [
                    '*.csv',
                    'data.csv',
                    'data',
                    '*.parquet',
                    '*.pq',
                    '*.npy'
                ]
                
                for pattern in patterns:
                    files = glob.glob(os.path.join(input_path, pattern))
                    if files:
                        data_file = files[0]
                        print(f"Found data file using pattern '{pattern}': {data_file}")
                        return data_file
                
                # Last resort: get first non-hidden file
                all_files = [
                    f for f in os.listdir(input_path)
                    if os.path.isfile(os.path.join(input_path, f))
                    and not f.startswith('.')
                ]
                
                if all_files:
                    data_file = os.path.join(input_path, all_files[0])
                    print(f"Using first available file: {data_file}")
                    return data_file
                
                raise FileNotFoundError(f"No data file found in directory: {input_path}")
            
            raise FileNotFoundError(f"Input path not found: {input_path}")
        
        def load_data_file(file_path):
            #Load data from file with format auto-detection#
            print(f"Loading data from: {file_path}")
            
            # Detect format from extension or try reading
            ext = Path(file_path).suffix.lower()
            
            try:
                if ext == '.npy':
                    X = np.load(file_path)
                    print(f"Loaded NumPy array")
                    return X
                
                elif ext in ['.parquet', '.pq']:
                    df = pd.read_parquet(file_path)
                    print(f"Loaded Parquet file")
                    return df.values
                
                elif ext == '.csv' or ext == '':
                    # Try CSV first (most common)
                    try:
                        df = pd.read_csv(file_path)
                        print(f"Loaded CSV file")
                        return df.values
                    except Exception as csv_error:
                        # If CSV fails and no extension, try numpy
                        if ext == '':
                            print(f"CSV failed, trying NumPy array...")
                            X = np.load(file_path, allow_pickle=True)
                            print(f"Loaded as NumPy array")
                            return X
                        else:
                            raise csv_error
                
                else:
                    # Unknown extension, try CSV as fallback
                    print(f"Unknown extension '{ext}', attempting CSV...")
                    df = pd.read_csv(file_path)
                    return df.values
                    
            except Exception as e:
                raise ValueError(f"Failed to load data from {file_path}: {str(e)}")
        
        def load_algorithm_class(algorithm_name):
            #Dynamically load algorithm class#
            if algorithm_name not in ALGORITHM_REGISTRY:
                available = ', '.join(ALGORITHM_REGISTRY.keys())
                raise ValueError(f"Unknown algorithm: {algorithm_name}. Available: {available}")
            
            config = ALGORITHM_REGISTRY[algorithm_name]
            module_name = config['module']
            class_name = config['class']
            
            try:
                module = __import__(module_name, fromlist=[class_name])
                return getattr(module, class_name), config
            except ImportError as e:
                raise ImportError(f"Failed to import {class_name} from {module_name}: {e}")
        
        def auto_tune_parameters(algorithm_name, default_params, n_samples, n_features):
            #Auto-tune parameters based on data characteristics#
            params = default_params.copy()
            
            print(f"Auto-tuning parameters for {algorithm_name}...")
            print(f"Data shape: {n_samples} samples x {n_features} features")
            
            if algorithm_name in ['KMeans', 'MiniBatchKMeans', 'BisectingKMeans']:
                suggested_k = max(2, min(int(np.sqrt(n_samples / 2)), 50))
                params['n_clusters'] = suggested_k
                print(f"Auto-tuned n_clusters: {suggested_k}")
                
            elif algorithm_name == 'DBSCAN':
                suggested_eps = max(0.3, min(2.0, n_features * 0.1))
                suggested_min_samples = max(3, min(2 * n_features, 50))
                params['eps'] = suggested_eps
                params['min_samples'] = suggested_min_samples
                print(f"Auto-tuned eps: {suggested_eps:.3f}")
                print(f"Auto-tuned min_samples: {suggested_min_samples}")
                
            elif algorithm_name in ['GaussianMixture', 'BayesianGaussianMixture']:
                if n_features > 20:
                    suggested_comp = max(2, min(5, int(np.sqrt(n_features))))
                else:
                    suggested_comp = max(2, min(10, int(np.sqrt(n_samples / 10))))
                params['n_components'] = suggested_comp
                print(f"Auto-tuned n_components: {suggested_comp}")
            
            elif algorithm_name == 'SpectralClustering':
                suggested_k = max(2, min(int(np.sqrt(n_samples / 2)), 30))
                suggested_neighbors = max(5, min(int(np.log(n_samples)), 20))
                params['n_clusters'] = suggested_k
                params['n_neighbors'] = suggested_neighbors
                print(f"Auto-tuned n_clusters: {suggested_k}")
                print(f"Auto-tuned n_neighbors: {suggested_neighbors}")
            
            elif algorithm_name == 'AgglomerativeClustering':
                suggested_k = max(2, min(int(np.sqrt(n_samples / 2)), 30))
                params['n_clusters'] = suggested_k
                print(f"Auto-tuned n_clusters: {suggested_k}")
            
            return params
        
        def train_model(X_train, algorithm_name, parameters, training_mode='auto'):
            #Train clustering model using unified pipeline logic#
            print(f"Training {algorithm_name}...")
            print(f"Data shape: {X_train.shape}")
            print(f"Training mode: {training_mode}")
            
            AlgorithmClass, config = load_algorithm_class(algorithm_name)
            
            if training_mode == 'auto':
                training_mode = config['preferred_training']
                print(f"Auto-selected mode: {training_mode}")
            
            if training_mode == 'fit':
                print("Training with fit()...")
                model = AlgorithmClass(**parameters)
                model.fit(X_train)
                
                if config['has_labels']:
                    train_labels = model.labels_
                else:
                    train_labels = model.predict(X_train)
            
            elif training_mode == 'fit_predict':
                print("Training with fit_predict()...")
                model = AlgorithmClass(**parameters)
                train_labels = model.fit_predict(X_train)
            
            else:
                raise ValueError(f"Unknown training_mode: {training_mode}")
            
            unique_labels = np.unique(train_labels)
            n_clusters = len(unique_labels[unique_labels != -1])
            n_noise = np.sum(train_labels == -1)
            
            print(f"Training Results:")
            print(f"  Clusters found: {n_clusters}")
            
            if n_noise > 0:
                print(f"  Noise points: {n_noise} ({n_noise/len(train_labels)*100:.1f}%)")
            
            print(f"  Cluster distribution:")
            for label in sorted(unique_labels):
                count = np.sum(train_labels == label)
                pct = count / len(train_labels) * 100
                label_name = "Noise" if label == -1 else f"Cluster {label}"
                print(f"    {label_name}: {count:>5} samples ({pct:>5.1f}%)")
            
            return model, train_labels, config
        
        def save_outputs(model, train_labels, X_train, config, parameters, algorithm_name, output_model, output_labels, output_metadata):
            #Save model and training results#
            print(f"Saving outputs...")
            
            os.makedirs(os.path.dirname(output_model), exist_ok=True)
            os.makedirs(os.path.dirname(output_labels), exist_ok=True)
            os.makedirs(os.path.dirname(output_metadata), exist_ok=True)
            
            # Save model as pickle
            with open(output_model, 'wb') as f:
                model_data = {
                    'model': model,
                    'train_data': X_train,
                    'algorithm': algorithm_name,
                    'config': config
                }
                pickle.dump(model_data, f)
            print(f"Model saved: {output_model}")
            
            # Save labels as CSV for consistency with pipeline
            labels_df = pd.DataFrame({
                'sample_index': np.arange(len(train_labels)),
                'cluster_label': train_labels
            })
            labels_df.to_csv(output_labels, index=False)
            print(f"Labels saved: {output_labels}")
            
            # Save metadata as JSON
            unique_labels = np.unique(train_labels)
            n_clusters = len(unique_labels[unique_labels != -1])
            n_noise = int(np.sum(train_labels == -1))
            
            metadata = {
                'algorithm': algorithm_name,
                'category': config['category'],
                'parameters': parameters,
                'capabilities': {
                    'has_predict': config.get('has_predict', False),
                    'allows_noise': config.get('allows_noise', False),
                    'preferred_inference': config.get('preferred_inference', 'native')
                },
                'training_results': {
                    'n_train_samples': int(X_train.shape[0]),
                    'n_features': int(X_train.shape[1]),
                    'n_clusters': int(n_clusters),
                    'n_noise': n_noise,
                    'noise_percentage': float(n_noise / len(train_labels) * 100),
                    'cluster_sizes': {
                        int(label): int(np.sum(train_labels == label))
                        for label in unique_labels
                    }
                },
                'training_complete': True
            }
            
            with open(output_metadata, 'w') as f:
                json.dump(metadata, f, indent=2)
            print(f"Metadata saved: {output_metadata}")
            
            return metadata
        
        parser = argparse.ArgumentParser(description="Train clustering model")
        parser.add_argument("--train_data", required=True)
        parser.add_argument("--algorithm", default='KMeans')
        parser.add_argument("--parameters", default='{}')
        parser.add_argument("--auto_tune", default='false')
        parser.add_argument("--training_mode", default='auto')
        parser.add_argument("--output_model", required=True)
        parser.add_argument("--output_labels", required=True)
        parser.add_argument("--output_metadata", required=True)
        args = parser.parse_args()
        
        print("="*80)
        print("MODEL TRAINING COMPONENT")
        print("="*80)
        print(f"Algorithm: {args.algorithm}")
        print(f"Auto-tune: {args.auto_tune}")
        print(f"Training mode: {args.training_mode}")
        print("")
        
        try:
            # Load training data using robust file discovery
            data_file = find_data_file(args.train_data)
            X_train = load_data_file(data_file)
            
            print(f"Loaded successfully")
            print(f"Shape: {X_train.shape[0]} samples x {X_train.shape[1]} features")
            
            if X_train.shape[0] < 2:
                raise ValueError(f"Need at least 2 samples, got {X_train.shape[0]}")
            
            if not np.isfinite(X_train).all():
                n_invalid = (~np.isfinite(X_train)).sum()
                raise ValueError(f"Data contains {n_invalid} NaN/Inf values")
            
            # Get algorithm configuration
            if args.algorithm not in ALGORITHM_REGISTRY:
                available = ', '.join(ALGORITHM_REGISTRY.keys())
                raise ValueError(f"Unknown algorithm: {args.algorithm}. Available: {available}")
            
            config = ALGORITHM_REGISTRY[args.algorithm]
            
            print(f"Algorithm category: {config['category']}")
            print(f"Training method: {config['preferred_training']}")
            
            # Prepare parameters
            parameters = config['default_params'].copy()
            
            print("Configuring parameters...")
            print(f"Starting with defaults for {args.algorithm}")
            
            if args.auto_tune.lower() == 'true':
                parameters = auto_tune_parameters(
                    args.algorithm,
                    parameters,
                    X_train.shape[0],
                    X_train.shape[1]
                )
            
            if args.parameters and args.parameters != '{}':
                try:
                    # Try JSON first (preferred)
                    user_params = json.loads(args.parameters)
                except json.JSONDecodeError:
                    # Fallback: try Python literal (handles single quotes)
                    try:
                        import ast
                        user_params = ast.literal_eval(args.parameters)
                        if not isinstance(user_params, dict):
                            raise ValueError("Parameters must be a dictionary")
                        print("Note: Parsed Python dict syntax (use JSON with double quotes for best compatibility)")
                    except (ValueError, SyntaxError) as e:
                        raise ValueError(
                            f"Invalid parameters format. Use JSON like: "
                            f'{{"n_clusters": 3}} or Python dict like: '
                            f"{{'n_clusters': 3}}. Error: {e}"
                        )
                
                print(f"Applying {len(user_params)} user parameters:")
                for key, value in user_params.items():
                    if key in parameters:
                        old_value = parameters[key]
                        parameters[key] = value
                        print(f"  {key}: {old_value} -> {value}")
                    else:
                        print(f"  Warning: Unknown parameter '{key}' (ignored)")
            
            print("Final parameters:")
            for key, value in parameters.items():
                print(f"  {key}: {value}")
            
            # Train model
            model, train_labels, config = train_model(
                X_train=X_train,
                algorithm_name=args.algorithm,
                parameters=parameters,
                training_mode=args.training_mode
            )
            
            # Save outputs
            metadata = save_outputs(
                model=model,
                train_labels=train_labels,
                X_train=X_train,
                config=config,
                parameters=parameters,
                algorithm_name=args.algorithm,
                output_model=args.output_model,
                output_labels=args.output_labels,
                output_metadata=args.output_metadata
            )
            
            print("")
            print("="*80)
            print("TRAINING COMPLETED SUCCESSFULLY")
            print("="*80)
            print(f"Algorithm: {args.algorithm}")
            print(f"Category: {config['category']}")
            print(f"Training samples: {X_train.shape[0]}")
            print(f"Features: {X_train.shape[1]}")
            print(f"Clusters found: {metadata['training_results']['n_clusters']}")
            
            if metadata['training_results']['n_noise'] > 0:
                print(f"Noise points: {metadata['training_results']['n_noise']} "
                      f"({metadata['training_results']['noise_percentage']:.1f}%)")
            
            print("="*80)
            
        except Exception as e:
            print(f"ERROR: {str(e)}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    args:
      - --train_data
      - {inputPath: train_data}
      - --algorithm
      - {inputValue: algorithm}
      - --parameters
      - {inputValue: parameters}
      - --auto_tune
      - {inputValue: auto_tune}
      - --training_mode
      - {inputValue: training_mode}
      - --output_model
      - {outputPath: model}
      - --output_labels
      - {outputPath: labels}
      - --output_metadata
      - {outputPath: metadata}
