name: Data Loading Component
description: Load and validate data for clustering pipelines. Supports ground truth extraction for supervised evaluation.

inputs:
  - name: input_dataset
    type: Dataset
    description: 'Input dataset from previous component (directory containing data file)'
  - name: file_format
    type: String
    description: 'File format (auto, csv, parquet, excel, json)'
    default: 'auto'
  - name: encoding
    type: String
    description: 'File encoding for CSV files'
    default: 'utf-8'
  - name: sep
    type: String
    description: 'CSV separator character'
    default: ','
  - name: target_column
    type: String
    description: 'Target column name for ground truth (optional, e.g., "species")'
    default: ''
  - name: target_mapping
    type: String
    description: 'JSON mapping for categorical targets (e.g., {"setosa": 0, "versicolor": 1})'
    default: '{}'

outputs:
  - name: data
    type: Data
    description: 'Loaded and validated dataset (WITHOUT target column)'
  - name: metadata
    type: Data
    description: 'Dataset metadata and validation results'
  - name: ground_truth
    type: Data
    description: 'Ground truth labels if target_column specified'

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        pip install pandas==2.0.3 numpy==1.24.3 pyarrow==12.0.1 openpyxl==3.1.2 && exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import json
        import pandas as pd
        import numpy as np
        from pathlib import Path
        import logging
        import glob
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('data_loading')
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
        
        def find_data_file(input_path):
            logger.info(f"Searching for data file in: {input_path}")
            
            if os.path.isfile(input_path):
                logger.info(f"Input is a file: {input_path}")
                return input_path
            
            if os.path.isdir(input_path):
                logger.info(f"Input is a directory, searching for data file...")
                patterns = ['*.csv', '*.parquet', '*.pq', '*.xlsx', '*.xls', '*.json']
                
                for pattern in patterns:
                    files = glob.glob(os.path.join(input_path, pattern))
                    if files:
                        data_file = files[0]
                        logger.info(f"Found data file: {data_file}")
                        return data_file
                
                all_files = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]
                if all_files:
                    data_file = os.path.join(input_path, all_files[0])
                    logger.info(f"No standard extension found, using first file: {data_file}")
                    return data_file
                
                raise FileNotFoundError(f"No data file found in directory: {input_path}")
            
            raise FileNotFoundError(f"Input path not found: {input_path}")
        
        def load_data(input_path, file_format='auto', encoding='utf-8', sep=','):
            logger.info(f"Loading data from: {input_path}")
            
            if file_format == 'auto':
                ext = Path(input_path).suffix.lower()
                format_map = {
                    '.csv': 'csv',
                    '.parquet': 'parquet',
                    '.pq': 'parquet',
                    '.xlsx': 'excel',
                    '.xls': 'excel',
                    '.json': 'json'
                }
                file_format = format_map.get(ext, 'csv')
            
            logger.info(f"Format: {file_format}")
            
            if file_format == 'csv':
                df = pd.read_csv(input_path, encoding=encoding, sep=sep)
            elif file_format == 'parquet':
                df = pd.read_parquet(input_path)
            elif file_format == 'excel':
                df = pd.read_excel(input_path)
            elif file_format == 'json':
                df = pd.read_json(input_path)
            else:
                raise ValueError(f"Unsupported format: {file_format}")
            
            logger.info(f"Loaded successfully: {df.shape[0]} rows x {df.shape[1]} columns")
            return df
        
        def extract_ground_truth(df, target_column, target_mapping_json):
            """Extract and encode ground truth labels"""
            
            if not target_column or target_column == '':
                logger.info("No target column specified - skipping ground truth extraction")
                return df, None, None
            
            logger.info(f"Extracting ground truth from column: '{target_column}'")
            
            # Check if target column exists
            if target_column not in df.columns:
                available_cols = ', '.join(df.columns.tolist())
                raise ValueError(
                    f"Target column '{target_column}' not found in dataset. "
                    f"Available columns: {available_cols}"
                )
            
            # Extract target column
            target_series = df[target_column].copy()
            logger.info(f"Found {len(target_series)} target values")
            
            # Parse mapping if provided
            target_mapping = {}
            if target_mapping_json and target_mapping_json != '{}':
                try:
                    target_mapping = json.loads(target_mapping_json)
                    logger.info(f"Using custom mapping: {target_mapping}")
                except json.JSONDecodeError:
                    logger.warning(f"Invalid target_mapping JSON, will auto-encode")
                    target_mapping = {}
            
            # Encode target labels
            ground_truth_info = {
                'column_name': target_column,
                'n_samples': int(len(target_series)),
                'unique_values': target_series.unique().tolist(),
                'n_unique': int(target_series.nunique()),
                'dtype': str(target_series.dtype)
            }
            
            # Check if already numeric
            if pd.api.types.is_numeric_dtype(target_series):
                logger.info("Target column is already numeric")
                ground_truth_encoded = target_series.values
                ground_truth_info['encoding'] = 'none (already numeric)'
                ground_truth_info['mapping'] = {}
            
            # Apply custom mapping
            elif target_mapping:
                logger.info("Applying custom mapping to target column")
                ground_truth_encoded = target_series.map(target_mapping).values
                
                # Check for unmapped values
                if pd.isna(ground_truth_encoded).any():
                    unmapped = target_series[pd.isna(ground_truth_encoded)].unique()
                    raise ValueError(
                        f"Custom mapping incomplete. Unmapped values: {unmapped.tolist()}"
                    )
                
                ground_truth_info['encoding'] = 'custom'
                ground_truth_info['mapping'] = target_mapping
            
            # Auto-encode categorical
            else:
                logger.info("Auto-encoding categorical target column")
                unique_vals = sorted(target_series.unique())
                auto_mapping = {val: idx for idx, val in enumerate(unique_vals)}
                ground_truth_encoded = target_series.map(auto_mapping).values
                
                ground_truth_info['encoding'] = 'auto (alphabetical)'
                ground_truth_info['mapping'] = auto_mapping
                logger.info(f"Auto-generated mapping: {auto_mapping}")
            
            # Validate encoding
            ground_truth_encoded = ground_truth_encoded.astype(int)
            ground_truth_info['encoded_range'] = [int(ground_truth_encoded.min()), int(ground_truth_encoded.max())]
            ground_truth_info['encoded_unique'] = int(len(np.unique(ground_truth_encoded)))
            
            # Log distribution
            logger.info("Ground truth distribution:")
            for label in np.unique(ground_truth_encoded):
                count = np.sum(ground_truth_encoded == label)
                pct = count / len(ground_truth_encoded) * 100
                logger.info(f"  Class {label}: {count} samples ({pct:.1f}%)")
            
            # Remove target column from features
            df_features = df.drop(columns=[target_column])
            logger.info(f"Removed target column from features: {df_features.shape[1]} features remain")
            
            return df_features, ground_truth_encoded, ground_truth_info
        
        def validate_data(df):
            logger.info("Validating data...")
            
            validation = {
                'is_valid': True,
                'errors': [],
                'warnings': []
            }
            
            if df.empty:
                validation['is_valid'] = False
                validation['errors'].append("Dataset is empty")
                return validation, {}
            
            logger.info(f"Dataset has {len(df)} samples")
            
            if len(df) < 2:
                validation['is_valid'] = False
                validation['errors'].append("Need at least 2 samples")
                return validation, {}
            
            if df.columns.duplicated().any():
                validation['warnings'].append("Duplicate column names")
                logger.warning("Duplicate column names detected")
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
            
            logger.info(f"Numeric columns: {len(numeric_cols)}")
            logger.info(f"Categorical columns: {len(categorical_cols)}")
            
            if len(numeric_cols) == 0:
                validation['warnings'].append("No numeric columns found")
                logger.warning("No numeric columns for clustering")
            
            missing_total = df.isnull().sum().sum()
            missing_pct = (missing_total / (df.shape[0] * df.shape[1])) * 100
            
            missing_by_col = {}
            if missing_total > 0:
                msg = f"Found {missing_total} missing values ({missing_pct:.2f}%)"
                validation['warnings'].append(msg)
                logger.warning(msg)
                for col in df.columns:
                    miss_count = df[col].isnull().sum()
                    if miss_count > 0:
                        missing_by_col[col] = int(miss_count)
                        logger.info(f"  - {col}: {miss_count} ({miss_count/len(df)*100:.1f}%)")
            else:
                logger.info("No missing values")
            
            inf_count = 0
            for col in numeric_cols:
                col_inf = df[col].isin([np.inf, -np.inf]).sum()
                inf_count += col_inf
            
            if inf_count > 0:
                msg = f"Found {inf_count} infinite values"
                validation['warnings'].append(msg)
                logger.warning(msg)
            else:
                logger.info("No infinite values")
            
            constant_cols = [col for col in df.columns if df[col].nunique() <= 1]
            if constant_cols:
                msg = f"Constant columns: {constant_cols}"
                validation['warnings'].append(msg)
                logger.warning(msg)
            else:
                logger.info("No constant columns")
            
            metadata = {
                'n_samples': int(df.shape[0]),
                'n_features': int(df.shape[1]),
                'columns': list(df.columns),
                'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()},
                'numeric_columns': numeric_cols,
                'categorical_columns': categorical_cols,
                'missing_values': {
                    'total': int(missing_total),
                    'percentage': float(missing_pct),
                    'by_column': missing_by_col
                },
                'constant_columns': constant_cols,
                'memory_mb': float(df.memory_usage(deep=True).sum() / 1024**2)
            }
            
            if numeric_cols:
                metadata['statistics'] = {}
                for col in numeric_cols:
                    try:
                        metadata['statistics'][col] = {
                            'mean': float(df[col].mean()),
                            'std': float(df[col].std()),
                            'min': float(df[col].min()),
                            'max': float(df[col].max()),
                            'median': float(df[col].median()),
                            'q25': float(df[col].quantile(0.25)),
                            'q75': float(df[col].quantile(0.75))
                        }
                    except:
                        pass
            
            metadata['validation'] = validation
            return validation, metadata
        
        parser = argparse.ArgumentParser(description="Load and validate data for clustering")
        parser.add_argument("--input_dataset", required=True)
        parser.add_argument("--file_format", default='auto')
        parser.add_argument("--encoding", default='utf-8')
        parser.add_argument("--sep", default=',')
        parser.add_argument("--target_column", default='')
        parser.add_argument("--target_mapping", default='{}')
        parser.add_argument("--output_data", required=True)
        parser.add_argument("--output_metadata", required=True)
        parser.add_argument("--output_ground_truth", required=True)
        args = parser.parse_args()
        
        logger.info("="*80)
        logger.info("DATA LOADING COMPONENT (WITH GROUND TRUTH SUPPORT)")
        logger.info("="*80)
        logger.info(f"Input dataset: {args.input_dataset}")
        logger.info(f"Format: {args.file_format}")
        logger.info(f"Target column: {args.target_column if args.target_column else 'None'}")
        logger.info("")
        
        try:
            ensure_directory_exists(args.output_data)
            ensure_directory_exists(args.output_metadata)
            ensure_directory_exists(args.output_ground_truth)
            
            # Load data
            data_file_path = find_data_file(args.input_dataset)
            df = load_data(data_file_path, args.file_format, args.encoding, args.sep)
            
            # Extract ground truth (if specified)
            df_features, ground_truth, ground_truth_info = extract_ground_truth(
                df, args.target_column, args.target_mapping
            )
            
            # Validate feature data
            validation, metadata = validate_data(df_features)
            
            if not validation['is_valid']:
                logger.error("VALIDATION FAILED")
                for error in validation['errors']:
                    logger.error(f"Error: {error}")
                sys.exit(1)
            
            if validation['warnings']:
                logger.warning(f"Warnings: {len(validation['warnings'])}")
                for warning in validation['warnings']:
                    logger.warning(f"  - {warning}")
            
            # Add ground truth info to metadata
            if ground_truth_info:
                metadata['ground_truth'] = ground_truth_info
            
            # Save feature data
            df_features.to_csv(args.output_data, index=False)
            logger.info(f"Feature data saved: {args.output_data}")
            
            # Save metadata
            with open(args.output_metadata, 'w') as f:
                json.dump(metadata, f, indent=2)
            logger.info(f"Metadata saved: {args.output_metadata}")
            
            # Save ground truth - FIXED: Use file handle to avoid .npy extension
            if ground_truth is not None:
                with open(args.output_ground_truth, 'wb') as f:
                    np.save(f, ground_truth, allow_pickle=False)
                logger.info(f"Ground truth saved: {args.output_ground_truth}")
                logger.info(f"Ground truth shape: {ground_truth.shape}")
                # Verify file exists
                if os.path.exists(args.output_ground_truth):
                    logger.info(f"Verified: Ground truth file exists at {args.output_ground_truth}")
                else:
                    logger.error(f"ERROR: Ground truth file was not created at {args.output_ground_truth}")
            else:
                # Create empty file to satisfy output requirement
                with open(args.output_ground_truth, 'w') as f:
                    f.write("")
                logger.info("No ground truth extracted (empty file created)")
            
            logger.info("")
            logger.info("="*80)
            logger.info("DATA LOADING COMPLETED SUCCESSFULLY")
            logger.info("="*80)
            logger.info(f"Samples: {metadata['n_samples']}")
            logger.info(f"Features: {metadata['n_features']}")
            logger.info(f"  - Numeric: {len(metadata['numeric_columns'])}")
            logger.info(f"  - Categorical: {len(metadata['categorical_columns'])}")
            if ground_truth_info:
                logger.info(f"Ground truth: {ground_truth_info['n_unique']} classes")
            logger.info("="*80)
            
        except FileNotFoundError as e:
            logger.error(f"ERROR: {str(e)}")
            sys.exit(1)
        except Exception as e:
            logger.error(f"ERROR: {str(e)}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    args:
      - --input_dataset
      - {inputPath: input_dataset}
      - --file_format
      - {inputValue: file_format}
      - --encoding
      - {inputValue: encoding}
      - --sep
      - {inputValue: sep}
      - --target_column
      - {inputValue: target_column}
      - --target_mapping
      - {inputValue: target_mapping}
      - --output_data
      - {outputPath: data}
      - --output_metadata
      - {outputPath: metadata}
      - --output_ground_truth
      - {outputPath: ground_truth}
