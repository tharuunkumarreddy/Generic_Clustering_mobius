name: Data Preprocessing Component
description: Comprehensive preprocessing for clustering algorithms. Handles categorical columns, missing values, outliers, scaling, and train-test split.

inputs:
  - name: input_data
    type: Data
    description: Input data from data loading component
  - name: test_size
    type: Float
    description: Test split ratio (0-1)
    default: 0.2
  - name: random_state
    type: Integer
    description: Random seed for reproducibility
    default: 42
  - name: scaling
    type: String
    description: Scaling method - standard, minmax, robust, none
    default: standard
  - name: missing
    type: String
    description: Missing value handling - mean, median, drop, zero
    default: mean
  - name: categorical
    type: String
    description: Categorical handling - encode, drop, onehot
    default: encode
  - name: outliers
    type: String
    description: Outlier handling - clip, remove, none
    default: clip
  - name: outlier_threshold
    type: Float
    description: Z-score threshold for outliers
    default: 3.0
  - name: remove_constant
    type: Boolean
    description: Remove constant or low variance columns
    default: true

outputs:
  - name: output_train
    type: Data
    description: Preprocessed training data
  - name: output_test
    type: Data
    description: Preprocessed test data
  - name: output_scaler
    type: Data
    description: Fitted scaler and encoders pickle file
  - name: output_report
    type: Data
    description: Preprocessing report JSON

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        pip install pandas==2.0.3 numpy==1.24.3 scikit-learn==1.3.0 scipy==1.11.1 && exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import pickle
        import argparse
        import warnings
        import pandas as pd
        import numpy as np
        from pathlib import Path
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder
        from sklearn.impute import SimpleImputer
        from scipy import stats
        
        warnings.filterwarnings('ignore')
        
        class ClusteringPreprocessor:
            def __init__(self):
                self.scaler = None
                self.label_encoders = {}
                self.dropped_columns = []
                self.original_columns = []
                self.final_columns = []
                self.preprocessing_report = {}
            
            def load_data(self, input_path):
                print(f"Loading input data from: {input_path}")
                try:
                    df = pd.read_csv(input_path)
                    print(f"Loaded successfully")
                    print(f"Shape: {df.shape[0]} rows x {df.shape[1]} columns")
                    self.original_columns = list(df.columns)
                    return df
                except Exception as e:
                    print(f"Error loading data: {str(e)}")
                    raise
            
            def analyze_columns(self, df):
                print(f"Analyzing columns...")
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
                print(f"Numeric columns: {len(numeric_cols)}")
                for col in numeric_cols:
                    print(f"  - {col} ({df[col].dtype})")
                if categorical_cols:
                    print(f"Categorical columns: {len(categorical_cols)}")
                    for col in categorical_cols:
                        unique_count = df[col].nunique()
                        print(f"  - {col} ({df[col].dtype}, {unique_count} unique values)")
                else:
                    print(f"No categorical columns")
                return numeric_cols, categorical_cols
            
            def handle_categorical_columns(self, df, categorical_cols, action='encode', max_categories=10):
                if not categorical_cols:
                    return df
                print(f"Handling categorical columns (action: {action})...")
                df_processed = df.copy()
                for col in categorical_cols:
                    unique_count = df[col].nunique()
                    if action == 'drop':
                        df_processed = df_processed.drop(columns=[col])
                        self.dropped_columns.append(col)
                        print(f"Dropped: {col}")
                    elif action == 'encode':
                        try:
                            le = LabelEncoder()
                            df_processed[col] = le.fit_transform(df[col].astype(str))
                            self.label_encoders[col] = le
                            print(f"Label encoded: {col} ({unique_count} categories)")
                        except Exception as e:
                            print(f"Could not encode {col}, dropping it: {str(e)}")
                            df_processed = df_processed.drop(columns=[col])
                            self.dropped_columns.append(col)
                    elif action == 'onehot':
                        if unique_count <= max_categories:
                            dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)
                            df_processed = pd.concat([df_processed.drop(columns=[col]), dummies], axis=1)
                            print(f"One-hot encoded: {col} ({unique_count} categories)")
                        else:
                            print(f"Too many categories in {col} ({unique_count}), dropping it")
                            df_processed = df_processed.drop(columns=[col])
                            self.dropped_columns.append(col)
                return df_processed
            
            def handle_missing_values(self, df, method='mean'):
                print(f"Handling missing values (method: {method})...")
                missing_before = df.isnull().sum().sum()
                missing_pct = (missing_before / (df.shape[0] * df.shape[1])) * 100
                print(f"Missing values: {missing_before} ({missing_pct:.2f}%)")
                if missing_before == 0:
                    print(f"No missing values")
                    return df
                missing_cols = df.columns[df.isnull().any()].tolist()
                for col in missing_cols:
                    miss_count = df[col].isnull().sum()
                    miss_pct = (miss_count / len(df)) * 100
                    print(f"  - {col}: {miss_count} ({miss_pct:.1f}%)")
                original_shape = df.shape
                if method == 'drop':
                    df_cleaned = df.dropna()
                    rows_dropped = original_shape[0] - df_cleaned.shape[0]
                    print(f"Dropped {rows_dropped} rows")
                elif method == 'zero':
                    df_cleaned = df.fillna(0)
                    print(f"Filled with 0")
                elif method in ['mean', 'median']:
                    imputer = SimpleImputer(strategy=method)
                    df_cleaned = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)
                    print(f"Imputed with {method}")
                else:
                    raise ValueError(f"Unknown method: {method}")
                return df_cleaned
            
            def remove_constant_columns(self, df, variance_threshold=0.0):
                print(f"Checking for constant or low-variance columns...")
                constant_cols = []
                for col in df.columns:
                    if df[col].nunique() <= 1:
                        constant_cols.append(col)
                        print(f"Constant column (will remove): {col}")
                    elif df[col].var() < variance_threshold and variance_threshold > 0:
                        constant_cols.append(col)
                        print(f"Low variance column (will remove): {col} (var={df[col].var():.6f})")
                if constant_cols:
                    df_filtered = df.drop(columns=constant_cols)
                    self.dropped_columns.extend(constant_cols)
                    print(f"Removed {len(constant_cols)} constant or low-variance columns")
                    return df_filtered
                else:
                    print(f"No constant columns")
                    return df
            
            def handle_outliers(self, df, method='clip', threshold=3.0):
                print(f"Handling outliers (method: {method}, threshold: {threshold})...")
                if method == 'none':
                    print(f"Skipping outlier handling")
                    return df
                df_processed = df.copy()
                for col in df.columns:
                    z_scores = np.abs(stats.zscore(df_processed[col]))
                    outliers = (z_scores > threshold).sum()
                    if outliers > 0:
                        outlier_pct = (outliers / len(df)) * 100
                        print(f"Column {col}: {outliers} outliers ({outlier_pct:.1f}%)")
                        if method == 'clip':
                            mean = df_processed[col].mean()
                            std = df_processed[col].std()
                            lower = mean - threshold * std
                            upper = mean + threshold * std
                            df_processed[col] = df_processed[col].clip(lower, upper)
                            print(f"  Clipped to [{lower:.3f}, {upper:.3f}]")
                if method == 'remove':
                    z_scores = np.abs(stats.zscore(df_processed))
                    df_processed = df_processed[(z_scores < threshold).all(axis=1)]
                    rows_removed = len(df) - len(df_processed)
                    print(f"Removed {rows_removed} rows with outliers")
                return df_processed
            
            def check_data_quality(self, df):
                print(f"Data Quality Check...")
                issues = []
                if len(df) < 10:
                    issues.append(f"Very few samples: {len(df)}")
                    print(f"Very few samples: {len(df)}")
                else:
                    print(f"Sufficient samples: {len(df)}")
                if df.shape[1] < 2:
                    issues.append(f"Less than 2 features: {df.shape[1]}")
                    print(f"Less than 2 features: {df.shape[1]}")
                else:
                    print(f"Sufficient features: {df.shape[1]}")
                inf_count = np.isinf(df.values).sum()
                if inf_count > 0:
                    issues.append(f"Found {inf_count} infinite values")
                    print(f"Infinite values: {inf_count}")
                    df = df.replace([np.inf, -np.inf], np.nan)
                    for col in df.columns:
                        if df[col].isnull().any():
                            df[col].fillna(df[col].median(), inplace=True)
                    print(f"Replaced infinite values with median")
                else:
                    print(f"No infinite values")
                nan_count = df.isnull().sum().sum()
                if nan_count > 0:
                    issues.append(f"Still has {nan_count} NaN values")
                    print(f"NaN values remaining: {nan_count}")
                else:
                    print(f"No NaN values")
                non_numeric = df.select_dtypes(exclude=[np.number]).columns.tolist()
                if non_numeric:
                    issues.append(f"Non-numeric columns: {non_numeric}")
                    print(f"Non-numeric columns: {non_numeric}")
                else:
                    print(f"All columns are numeric")
                return df, issues
            
            def split_data(self, df, test_size=0.2, random_state=42):
                print(f"Splitting data (test_size: {test_size})...")
                if test_size == 0:
                    print(f"No test split (all data for training)")
                    return df, pd.DataFrame(columns=df.columns)
                X_train, X_test = train_test_split(df, test_size=test_size, random_state=random_state)
                print(f"Train: {len(X_train)} samples ({(1-test_size)*100:.1f}%)")
                print(f"Test: {len(X_test)} samples ({test_size*100:.1f}%)")
                return X_train, X_test
            
            def scale_data(self, X_train, X_test, method='standard'):
                print(f"Scaling features (method: {method})...")
                if method == 'none':
                    print(f"No scaling applied")
                    self.scaler = None
                    return X_train, X_test
                if method == 'standard':
                    self.scaler = StandardScaler()
                    print(f"Using StandardScaler (mean=0, std=1)")
                elif method == 'minmax':
                    self.scaler = MinMaxScaler()
                    print(f"Using MinMaxScaler (range: 0-1)")
                elif method == 'robust':
                    self.scaler = RobustScaler()
                    print(f"Using RobustScaler (median-based, robust to outliers)")
                else:
                    raise ValueError(f"Unknown scaling method: {method}")
                X_train_scaled = self.scaler.fit_transform(X_train)
                X_test_scaled = self.scaler.transform(X_test) if not X_test.empty else X_test.values
                X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)
                X_test_df = pd.DataFrame(X_test_scaled, columns=X_test.columns) if not X_test.empty else X_test
                print(f"Scaling completed")
                print(f"Sample statistics (first 3 features):")
                for col in X_train_df.columns[:3]:
                    print(f"  {col}: mean={X_train_df[col].mean():.3f}, std={X_train_df[col].std():.3f}")
                return X_train_df, X_test_df
            
            def save_outputs(self, X_train, X_test, output_train, output_test, output_scaler, output_report):
                print(f"Saving outputs...")
                for path in [output_train, output_test, output_scaler, output_report]:
                    os.makedirs(os.path.dirname(path) or '.', exist_ok=True)
                X_train.to_csv(output_train, index=False)
                print(f"Train data: {output_train}")
                print(f"  Shape: {X_train.shape}")
                if not X_test.empty:
                    X_test.to_csv(output_test, index=False)
                    print(f"Test data: {output_test}")
                    print(f"  Shape: {X_test.shape}")
                else:
                    pd.DataFrame(columns=X_train.columns).to_csv(output_test, index=False)
                    print(f"Test data (empty): {output_test}")
                with open(output_scaler, 'wb') as f:
                    pickle.dump({'scaler': self.scaler, 'label_encoders': self.label_encoders, 'final_columns': self.final_columns}, f)
                print(f"Scaler and encoders: {output_scaler}")
                with open(output_report, 'w') as f:
                    json.dump(self.preprocessing_report, f, indent=2)
                print(f"Preprocessing report: {output_report}")
        
        def run_preprocessing(input_path, output_train, output_test, output_scaler, output_report, test_size=0.2, random_state=42, scaling='standard', missing='mean', categorical='encode', outliers='clip', outlier_threshold=3.0, remove_constant=True):
            preprocessor = ClusteringPreprocessor()
            df = preprocessor.load_data(input_path)
            original_shape = df.shape
            numeric_cols, categorical_cols = preprocessor.analyze_columns(df)
            df = preprocessor.handle_categorical_columns(df, categorical_cols, action=categorical)
            df = preprocessor.handle_missing_values(df, method=missing)
            if remove_constant:
                df = preprocessor.remove_constant_columns(df)
            df = preprocessor.handle_outliers(df, method=outliers, threshold=outlier_threshold)
            df, issues = preprocessor.check_data_quality(df)
            preprocessor.final_columns = list(df.columns)
            X_train, X_test = preprocessor.split_data(df, test_size=test_size, random_state=random_state)
            X_train_scaled, X_test_scaled = preprocessor.scale_data(X_train, X_test, method=scaling)
            preprocessor.preprocessing_report = {
                'original_shape': list(original_shape),
                'final_shape': [len(X_train_scaled) + len(X_test_scaled), X_train_scaled.shape[1]],
                'original_columns': preprocessor.original_columns,
                'final_columns': preprocessor.final_columns,
                'dropped_columns': preprocessor.dropped_columns,
                'categorical_action': categorical,
                'missing_method': missing,
                'outlier_method': outliers,
                'scaling_method': scaling,
                'test_size': test_size,
                'train_samples': len(X_train_scaled),
                'test_samples': len(X_test_scaled),
                'quality_issues': issues
            }
            preprocessor.save_outputs(X_train_scaled, X_test_scaled, output_train, output_test, output_scaler, output_report)
            return len(X_train_scaled), len(X_test_scaled), preprocessor.preprocessing_report
        
        parser = argparse.ArgumentParser(description='Comprehensive preprocessing for clustering algorithms')
        parser.add_argument('--input_data', required=True)
        parser.add_argument('--test_size', type=float, default=0.2)
        parser.add_argument('--random_state', type=int, default=42)
        parser.add_argument('--scaling', default='standard')
        parser.add_argument('--missing', default='mean')
        parser.add_argument('--categorical', default='encode')
        parser.add_argument('--outliers', default='clip')
        parser.add_argument('--outlier_threshold', type=float, default=3.0)
        parser.add_argument('--remove_constant', type=lambda x: x.lower() == 'true', default=True)
        parser.add_argument('--output_train', required=True)
        parser.add_argument('--output_test', required=True)
        parser.add_argument('--output_scaler', required=True)
        parser.add_argument('--output_report', required=True)
        args = parser.parse_args()
        
        print("=" * 80)
        print("DATA PREPROCESSING FOR CLUSTERING")
        print("=" * 80)
        print(f"Configuration:")
        print(f"  Input: {args.input_data}")
        print(f"  Test size: {args.test_size}")
        print(f"  Scaling: {args.scaling}")
        print(f"  Missing: {args.missing}")
        print(f"  Categorical: {args.categorical}")
        print(f"  Outliers: {args.outliers} (threshold: {args.outlier_threshold})")
        print(f"  Remove constant: {args.remove_constant}")
        
        try:
            n_train, n_test, report = run_preprocessing(
                input_path=args.input_data,
                output_train=args.output_train,
                output_test=args.output_test,
                output_scaler=args.output_scaler,
                output_report=args.output_report,
                test_size=args.test_size,
                random_state=args.random_state,
                scaling=args.scaling,
                missing=args.missing,
                categorical=args.categorical,
                outliers=args.outliers,
                outlier_threshold=args.outlier_threshold,
                remove_constant=args.remove_constant
            )
            print("=" * 80)
            print("PREPROCESSING COMPLETED SUCCESSFULLY")
            print("=" * 80)
            print(f"Transformation Summary:")
            print(f"  Original: {report['original_shape'][0]} rows x {report['original_shape'][1]} cols")
            print(f"  Final: {report['final_shape'][0]} rows x {report['final_shape'][1]} cols")
            print(f"  Dropped columns: {len(report['dropped_columns'])}")
            if report['dropped_columns']:
                print(f"    {report['dropped_columns']}")
            print(f"  Train samples: {n_train}")
            print(f"  Test samples: {n_test}")
            print(f"  Final features: {report['final_columns'][:5]}")
            if report['quality_issues']:
                print(f"Quality Issues:")
                for issue in report['quality_issues']:
                    print(f"  - {issue}")
            print(f"Data is ready for clustering!")
            print("=" * 80)
            sys.exit(0)
        except Exception as e:
            print(f"ERROR: {str(e)}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    args:
      - --input_data
      - {inputPath: input_data}
      - --test_size
      - {inputValue: test_size}
      - --random_state
      - {inputValue: random_state}
      - --scaling
      - {inputValue: scaling}
      - --missing
      - {inputValue: missing}
      - --categorical
      - {inputValue: categorical}
      - --outliers
      - {inputValue: outliers}
      - --outlier_threshold
      - {inputValue: outlier_threshold}
      - --remove_constant
      - {inputValue: remove_constant}
      - --output_train
      - {outputPath: output_train}
      - --output_test
      - {outputPath: output_test}
      - --output_scaler
      - {outputPath: output_scaler}
      - --output_report
      - {outputPath: output_report}
