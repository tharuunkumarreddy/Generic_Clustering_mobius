name: Model Inference Component
description: Universal inference component supporting ALL clustering algorithms. Handles algorithms with and without native predict() using unified logic.

inputs:
  - name: model
    type: Model
    description: 'Trained clustering model from training component'
  - name: metadata
    type: Data
    description: 'Training metadata from training component'
  - name: test_data
    type: Data
    description: 'Test data from preprocessing component'
  - name: train_data
    type: Data
    description: 'Training data (optional, needed for nearest_neighbor/refit modes)'
    optional: true
  - name: train_labels
    type: Data
    description: 'Training labels (optional, needed for nearest_neighbor/refit modes)'
    optional: true
  - name: inference_mode
    type: String
    description: 'Inference method: auto, native, nearest_neighbor, refit, approximate_predict, function_call'
    default: 'auto'

outputs:
  - name: predictions
    type: Data
    description: 'Test cluster predictions'
  - name: prediction_metadata
    type: Data
    description: 'Prediction statistics and metadata'

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        pip install pandas==2.0.3 numpy==1.24.3 scikit-learn==1.3.0 scipy==1.11.1 && exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import json
        import pickle
        import argparse
        import warnings
        import numpy as np
        import pandas as pd
        import glob
        from pathlib import Path
        
        warnings.filterwarnings('ignore')
        
        def find_file(input_path, file_patterns=None):
            #Find file in directory or return path if it's a file#
            print(f"Searching for file in: {input_path}")
            
            if os.path.isfile(input_path):
                print(f"Input is a file: {input_path}")
                return input_path
            
            if os.path.isdir(input_path):
                print(f"Input is a directory, searching...")
                
                if file_patterns is None:
                    file_patterns = ['*']
                
                for pattern in file_patterns:
                    files = glob.glob(os.path.join(input_path, pattern))
                    if files:
                        file_path = files[0]
                        print(f"Found file: {file_path}")
                        return file_path
                
                all_files = [
                    f for f in os.listdir(input_path)
                    if os.path.isfile(os.path.join(input_path, f))
                    and not f.startswith('.')
                ]
                
                if all_files:
                    file_path = os.path.join(input_path, all_files[0])
                    print(f"Using first file: {file_path}")
                    return file_path
                
                raise FileNotFoundError(f"No file found in directory: {input_path}")
            
            raise FileNotFoundError(f"Path not found: {input_path}")
        
        def load_model(model_path):
            #Load trained clustering model#
            print(f"Loading model from: {model_path}")
            
            model_file = find_file(model_path, ['*.pkl', 'model.pkl', 'data'])
            
            with open(model_file, 'rb') as f:
                model_data = pickle.load(f)
            
            if isinstance(model_data, dict):
                model = model_data.get('model', model_data)
                algorithm = model_data.get('algorithm', 'Unknown')
                print(f"Model loaded: {algorithm}")
            else:
                model = model_data
                print(f"Model loaded: {type(model).__name__}")
            
            return model_data if isinstance(model_data, dict) else {'model': model_data}
        
        def load_metadata(metadata_path):
            #Load training metadata#
            print(f"Loading metadata from: {metadata_path}")
            
            metadata_file = find_file(metadata_path, ['*.json', 'metadata.json', 'data'])
            
            with open(metadata_file, 'r') as f:
                metadata = json.load(f)
            
            print(f"Algorithm: {metadata.get('algorithm', 'Unknown')}")
            print(f"Category: {metadata.get('category', 'Unknown')}")
            
            capabilities = metadata.get('capabilities', {})
            print(f"Has native predict: {capabilities.get('has_predict', False)}")
            print(f"Preferred inference: {capabilities.get('preferred_inference', 'unknown')}")
            
            return metadata
        
        def load_data(data_path):
            #Load test data#
            print(f"Loading test data from: {data_path}")
            
            data_file = find_file(data_path, ['*.csv', '*.npy', 'data'])
            
            if data_file.endswith('.csv'):
                df = pd.read_csv(data_file)
                data = df.values
            elif data_file.endswith('.npy'):
                data = np.load(data_file)
            else:
                df = pd.read_csv(data_file)
                data = df.values
            
            print(f"Loaded: {data.shape[0]} samples x {data.shape[1]} features")
            
            if not np.isfinite(data).all():
                n_invalid = (~np.isfinite(data)).sum()
                raise ValueError(f"Data contains {n_invalid} NaN/Inf values")
            
            return data
        
        def load_training_data(train_data_path, train_labels_path):
            #Load training data and labels#
            print(f"Loading training reference data...")
            
            train_file = find_file(train_data_path, ['*.csv', '*.npy', 'data'])
            
            if train_file.endswith('.csv'):
                df = pd.read_csv(train_file)
                X_train = df.values
            elif train_file.endswith('.npy'):
                X_train = np.load(train_file)
            else:
                df = pd.read_csv(train_file)
                X_train = df.values
            
            print(f"Training data: {X_train.shape}")
            
            labels_file = find_file(train_labels_path, ['*.csv', '*.npy', 'data'])
            
            if labels_file.endswith('.csv'):
                df_labels = pd.read_csv(labels_file)
                if 'cluster_label' in df_labels.columns:
                    train_labels = df_labels['cluster_label'].values
                else:
                    train_labels = df_labels.iloc[:, -1].values
            elif labels_file.endswith('.npy'):
                train_labels = np.load(labels_file)
            else:
                df_labels = pd.read_csv(labels_file)
                if 'cluster_label' in df_labels.columns:
                    train_labels = df_labels['cluster_label'].values
                else:
                    train_labels = df_labels.iloc[:, -1].values
            
            print(f"Training labels: {len(train_labels)} samples")
            
            if len(X_train) != len(train_labels):
                raise ValueError(
                    f"Size mismatch: data={len(X_train)}, labels={len(train_labels)}"
                )
            
            return X_train, train_labels
        
        def validate_test_data(X_test, metadata):
            #Validate test data against training metadata#
            print(f"Validating test data...")
            
            expected_features = metadata['training_results']['n_features']
            actual_features = X_test.shape[1]
            
            if actual_features != expected_features:
                raise ValueError(
                    f"Feature mismatch: model expects {expected_features} features, "
                    f"test data has {actual_features} features"
                )
            
            print(f"Features match: {actual_features}")
            print(f"Test samples: {X_test.shape[0]}")
        
        def predict_native(model, X_test, algorithm_name):
            #Predict using native model.predict() method#
            print(f"[Method: Native Predict]")
            print(f"Using model.predict()...")
            
            predictions = model.predict(X_test)
            print(f"Predictions generated: {len(predictions)} samples")
            return predictions
        
        def predict_nearest_neighbor(model, X_test, X_train, train_labels, algorithm_name, metadata):
            #Predict using nearest neighbor assignment#
            print(f"[Method: Nearest Neighbor]")
            print(f"Finding nearest training sample for each test sample...")
            
            from sklearn.neighbors import NearestNeighbors
            
            nn = NearestNeighbors(n_neighbors=1, algorithm='auto')
            nn.fit(X_train)
            
            distances, indices = nn.kneighbors(X_test)
            predictions = train_labels[indices.flatten()]
            
            print(f"Assigned labels based on nearest neighbors")
            
            capabilities = metadata.get('capabilities', {})
            if capabilities.get('allows_noise', False):
                threshold = None
                
                if hasattr(model, 'eps'):
                    threshold = model.eps
                    threshold_name = 'eps'
                elif hasattr(model, 'max_eps'):
                    if model.max_eps != float('inf'):
                        threshold = model.max_eps
                        threshold_name = 'max_eps'
                
                if threshold is not None:
                    noise_mask = distances.flatten() > threshold
                    n_noise = noise_mask.sum()
                    if n_noise > 0:
                        predictions[noise_mask] = -1
                        print(f"Marked {n_noise} points as noise (distance > {threshold_name}={threshold:.4f})")
            
            return predictions
        
        def predict_refit(model, X_test, X_train, train_labels, algorithm_name, metadata):
            #Predict by refitting on combined train+test data#
            print(f"[Method: Refit]")
            print(f"Warning: Refitting model on combined data")
            print(f"Combining train and test data...")
            
            X_combined = np.vstack([X_train, X_test])
            print(f"Combined shape: {X_combined.shape}")
            
            if hasattr(model, 'fit_predict'):
                print(f"Using fit_predict()...")
                combined_labels = model.fit_predict(X_combined)
            elif hasattr(model, 'fit'):
                print(f"Using fit()...")
                model.fit(X_combined)
                if hasattr(model, 'labels_'):
                    combined_labels = model.labels_
                else:
                    combined_labels = model.predict(X_combined)
            else:
                raise ValueError(f"Cannot refit {algorithm_name}")
            
            test_predictions = combined_labels[len(X_train):]
            
            print(f"Refit complete")
            print(f"Note: Training cluster assignments may have changed")
            
            return test_predictions
        
        def run_inference(model, X_test, metadata, X_train=None, train_labels=None, inference_mode='auto'):
            #Run inference using appropriate method#
            
            if isinstance(model, dict):
                actual_model = model.get('model', model)
                algorithm_name = model.get('algorithm', metadata.get('algorithm', 'Unknown'))
            else:
                actual_model = model
                algorithm_name = metadata.get('algorithm', 'Unknown')
            
            capabilities = metadata.get('capabilities', {})
            
            print(f"Starting inference for {algorithm_name}...")
            print(f"Inference mode: {inference_mode}")
            
            if inference_mode == 'auto':
                inference_mode = capabilities.get('preferred_inference', 'native')
                print(f"Auto-selected mode: {inference_mode}")
            
            if inference_mode == 'native':
                if not capabilities.get('has_predict', False):
                    raise ValueError(
                        f"{algorithm_name} doesn't support native predict(). "
                        f"Try: nearest_neighbor or refit"
                    )
                predictions = predict_native(actual_model, X_test, algorithm_name)
            
            elif inference_mode == 'nearest_neighbor':
                if X_train is None or train_labels is None:
                    raise ValueError(
                        "Nearest neighbor inference requires training data"
                    )
                predictions = predict_nearest_neighbor(
                    actual_model, X_test, X_train, train_labels, algorithm_name, metadata
                )
            
            elif inference_mode == 'refit':
                if X_train is None or train_labels is None:
                    raise ValueError("Refit inference requires training data")
                predictions = predict_refit(
                    actual_model, X_test, X_train, train_labels, algorithm_name, metadata
                )
            
            else:
                raise ValueError(f"Unknown inference mode: {inference_mode}")
            
            return predictions
        
        def analyze_predictions(predictions):
            #Analyze prediction results#
            print(f"Analyzing predictions...")
            
            unique_labels = np.unique(predictions)
            n_clusters = len(unique_labels[unique_labels != -1])
            n_noise = np.sum(predictions == -1)
            
            stats = {
                'n_samples': int(len(predictions)),
                'n_clusters': int(n_clusters),
                'n_noise': int(n_noise),
                'noise_percentage': float(n_noise / len(predictions) * 100),
                'cluster_sizes': {}
            }
            
            print(f"Predictions: {len(predictions)} samples")
            print(f"Clusters: {n_clusters}")
            
            if n_noise > 0:
                print(f"Noise points: {n_noise} ({stats['noise_percentage']:.1f}%)")
            
            print(f"Cluster distribution:")
            for label in sorted(unique_labels):
                count = np.sum(predictions == label)
                pct = count / len(predictions) * 100
                label_name = "Noise" if label == -1 else f"Cluster {label}"
                stats['cluster_sizes'][int(label)] = int(count)
                print(f"  {label_name}: {count:>5} samples ({pct:>5.1f}%)")
            
            return stats
        
        def save_outputs(predictions, stats, metadata, output_predictions, output_metadata):
            #Save predictions and statistics#
            print(f"Saving outputs...")
            
            os.makedirs(os.path.dirname(output_predictions), exist_ok=True)
            os.makedirs(os.path.dirname(output_metadata), exist_ok=True)
            
            pred_df = pd.DataFrame({
                'sample_index': np.arange(len(predictions)),
                'cluster_label': predictions
            })
            pred_df.to_csv(output_predictions, index=False)
            print(f"Predictions saved: {output_predictions}")
            
            pred_metadata = {
                'algorithm': metadata.get('algorithm', 'Unknown'),
                'category': metadata.get('category', 'unknown'),
                'prediction_statistics': stats,
                'inference_complete': True
            }
            
            with open(output_metadata, 'w') as f:
                json.dump(pred_metadata, f, indent=2)
            print(f"Metadata saved: {output_metadata}")
            
            return pred_metadata
        
        parser = argparse.ArgumentParser(description="Inference for clustering models")
        parser.add_argument("--model", required=True)
        parser.add_argument("--metadata", required=True)
        parser.add_argument("--test_data", required=True)
        parser.add_argument("--train_data", default=None)
        parser.add_argument("--train_labels", default=None)
        parser.add_argument("--inference_mode", default='auto')
        parser.add_argument("--output_predictions", required=True)
        parser.add_argument("--output_prediction_metadata", required=True)
        args = parser.parse_args()
        
        print("="*80)
        print("MODEL INFERENCE COMPONENT")
        print("="*80)
        print(f"Inference mode: {args.inference_mode}")
        print("")
        
        try:
            model_data = load_model(args.model)
            metadata = load_metadata(args.metadata)
            X_test = load_data(args.test_data)
            
            validate_test_data(X_test, metadata)
            
            X_train = None
            train_labels = None
            
            if args.train_data and args.train_labels:
                if args.train_data.lower() not in ['none', '']:
                    X_train, train_labels = load_training_data(
                        args.train_data, 
                        args.train_labels
                    )
            elif args.inference_mode in ['nearest_neighbor', 'refit']:
                print(f"Warning: {args.inference_mode} mode works best with training data")
                print(f"Attempting inference without training data...")
            
            predictions = run_inference(
                model=model_data,
                X_test=X_test,
                metadata=metadata,
                X_train=X_train,
                train_labels=train_labels,
                inference_mode=args.inference_mode
            )
            
            stats = analyze_predictions(predictions)
            
            result_metadata = save_outputs(
                predictions=predictions,
                stats=stats,
                metadata=metadata,
                output_predictions=args.output_predictions,
                output_metadata=args.output_prediction_metadata
            )
            
            print("")
            print("="*80)
            print("INFERENCE COMPLETED SUCCESSFULLY")
            print("="*80)
            print(f"Algorithm: {metadata.get('algorithm', 'Unknown')}")
            print(f"Test samples: {X_test.shape[0]}")
            print(f"Clusters assigned: {stats['n_clusters']}")
            if stats['n_noise'] > 0:
                print(f"Noise points: {stats['n_noise']} ({stats['noise_percentage']:.1f}%)")
            print("="*80)
            
        except Exception as e:
            print(f"ERROR: {str(e)}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    args:
      - --model
      - {inputPath: model}
      - --metadata
      - {inputPath: metadata}
      - --test_data
      - {inputPath: test_data}
      - --train_data
      - {inputPath: train_data}
      - --train_labels
      - {inputPath: train_labels}
      - --inference_mode
      - {inputValue: inference_mode}
      - --output_predictions
      - {outputPath: predictions}
      - --output_prediction_metadata
      - {outputPath: prediction_metadata}
