name: Data Preprocessing Component
description: Comprehensive preprocessing for clustering algorithms. Handles missing values, categorical encoding, outlier detection, feature scaling, and train-test split.

inputs:
  - name: input_data
    type: Data
    description: 'Input data from data loading component'
  - name: test_size
    type: Float
    description: 'Test split proportion (0-1)'
    default: '0.2'
  - name: random_state
    type: Integer
    description: 'Random seed for reproducibility'
    default: '42'
  - name: scaling_method
    type: String
    description: 'Scaling method: standard, minmax, robust, none'
    default: 'standard'
  - name: missing_method
    type: String
    description: 'Missing value handling: mean, median, drop, zero'
    default: 'mean'
  - name: categorical_action
    type: String
    description: 'Categorical handling: encode, drop, onehot'
    default: 'drop'
  - name: outlier_method
    type: String
    description: 'Outlier handling: clip, remove, none'
    default: 'clip'
  - name: outlier_threshold
    type: Float
    description: 'Z-score threshold for outliers'
    default: '3.0'

outputs:
  - name: train
    type: Data
    description: 'Training dataset after preprocessing'
  - name: test
    type: Data
    description: 'Test dataset after preprocessing'
  - name: scaler
    type: Data
    description: 'Fitted scaler object'
  - name: report
    type: Data
    description: 'Preprocessing report with statistics'

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        pip install pandas==2.0.3 numpy==1.24.3 scikit-learn==1.3.0 scipy==1.11.1 && exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import json
        import pickle
        import warnings
        import pandas as pd
        import numpy as np
        import logging
        import glob
        from pathlib import Path
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder
        from sklearn.impute import SimpleImputer
        from scipy import stats
        
        warnings.filterwarnings('ignore')
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('preprocessing')
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
        
        def find_data_file(input_path):
            # Find the actual data file in the input path
            logger.info(f"Searching for data file in: {input_path}")
            
            if os.path.isfile(input_path):
                logger.info(f"Input is a file: {input_path}")
                return input_path
            
            if os.path.isdir(input_path):
                logger.info(f"Input is a directory, searching for data file...")
                patterns = ['*.csv', '*.parquet', '*.pq', 'data.csv', 'data']
                
                for pattern in patterns:
                    files = glob.glob(os.path.join(input_path, pattern))
                    if files:
                        data_file = files[0]
                        logger.info(f"Found data file: {data_file}")
                        return data_file
                
                all_files = [f for f in os.listdir(input_path) 
                            if os.path.isfile(os.path.join(input_path, f)) 
                            and not f.startswith('.')]
                if all_files:
                    data_file = os.path.join(input_path, all_files[0])
                    logger.info(f"Using first file: {data_file}")
                    return data_file
                
                raise FileNotFoundError(f"No data file found in directory: {input_path}")
            
            raise FileNotFoundError(f"Input path not found: {input_path}")
        
        def load_data(input_path):
            # Load data from CSV or parquet
            logger.info(f"Loading data from: {input_path}")
            
            if input_path.endswith(('.parquet', '.pq')):
                df = pd.read_parquet(input_path)
            else:
                df = pd.read_csv(input_path)
            
            logger.info(f"Loaded: {df.shape[0]} rows x {df.shape[1]} columns")
            return df
        
        def analyze_columns(df):
            # Analyze column types
            logger.info("Analyzing columns...")
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
            logger.info(f"Numeric columns: {len(numeric_cols)}")
            logger.info(f"Categorical columns: {len(categorical_cols)}")
            return numeric_cols, categorical_cols
        
        def handle_categorical(df, categorical_cols, action, dropped_cols, label_encoders):
            # Handle categorical columns based on specified action
            if not categorical_cols:
                return df
            
            logger.info(f"Handling categorical columns (action: {action})...")
            df_processed = df.copy()
            
            for col in categorical_cols:
                unique_count = df[col].nunique()
                
                if action == 'drop':
                    df_processed = df_processed.drop(columns=[col])
                    dropped_cols.append(col)
                    logger.info(f"Dropped: {col}")
                    
                elif action == 'encode':
                    try:
                        le = LabelEncoder()
                        df_processed[col] = le.fit_transform(df[col].astype(str))
                        label_encoders[col] = le
                        logger.info(f"Label encoded: {col} ({unique_count} categories)")
                    except Exception as e:
                        logger.warning(f"Could not encode {col}, dropping it: {e}")
                        df_processed = df_processed.drop(columns=[col])
                        dropped_cols.append(col)
                        
                elif action == 'onehot':
                    if unique_count <= 10:
                        dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)
                        df_processed = pd.concat([df_processed.drop(columns=[col]), dummies], axis=1)
                        logger.info(f"One-hot encoded: {col}")
                    else:
                        logger.warning(f"Too many categories in {col} ({unique_count}), dropping it")
                        df_processed = df_processed.drop(columns=[col])
                        dropped_cols.append(col)
            
            return df_processed
        
        def handle_missing(df, method):
            # Handle missing values based on specified method
            logger.info(f"Handling missing values (method: {method})...")
            missing_before = df.isnull().sum().sum()
            logger.info(f"Missing values: {missing_before}")
            
            if missing_before == 0:
                logger.info("No missing values")
                return df
            
            if method == 'drop':
                df_cleaned = df.dropna()
                logger.info(f"Dropped {len(df) - len(df_cleaned)} rows")
            elif method == 'zero':
                df_cleaned = df.fillna(0)
                logger.info("Filled with 0")
            elif method in ['mean', 'median']:
                imputer = SimpleImputer(strategy=method)
                df_cleaned = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)
                logger.info(f"Imputed with {method}")
            else:
                raise ValueError(f"Unknown method: {method}")
            
            return df_cleaned
        
        def remove_constant_columns(df, dropped_cols):
            # Remove columns with constant values
            logger.info("Checking for constant columns...")
            constant_cols = [col for col in df.columns if df[col].nunique() <= 1]
            
            if constant_cols:
                df_filtered = df.drop(columns=constant_cols)
                dropped_cols.extend(constant_cols)
                logger.info(f"Removed {len(constant_cols)} constant columns: {constant_cols}")
                return df_filtered
            else:
                logger.info("No constant columns")
                return df
        
        def handle_outliers(df, method, threshold):
            # Handle outliers using z-score method
            logger.info(f"Handling outliers (method: {method}, threshold: {threshold})...")
            
            if method == 'none':
                logger.info("Skipping outlier handling")
                return df
            
            df_processed = df.copy()
            outlier_counts = {}
            
            for col in df.columns:
                try:
                    z_scores = np.abs(stats.zscore(df_processed[col]))
                    outliers = (z_scores > threshold).sum()
                    
                    if outliers > 0:
                        outlier_counts[col] = outliers
                        if method == 'clip':
                            mean = df_processed[col].mean()
                            std = df_processed[col].std()
                            lower = mean - threshold * std
                            upper = mean + threshold * std
                            df_processed[col] = df_processed[col].clip(lower, upper)
                            logger.info(f"Column {col}: clipped {outliers} outliers")
                except Exception as e:
                    logger.warning(f"Could not process outliers for {col}: {e}")
            
            if method == 'remove':
                initial_len = len(df_processed)
                z_scores = np.abs(stats.zscore(df_processed))
                df_processed = df_processed[(z_scores < threshold).all(axis=1)]
                removed = initial_len - len(df_processed)
                logger.info(f"Removed {removed} rows with outliers")
            
            return df_processed
        
        def check_data_quality(df):
            # Perform comprehensive data quality checks
            logger.info("Data quality check...")
            issues = []
            
            if len(df) < 10:
                issues.append(f"Very few samples: {len(df)}")
                logger.warning(f"Very few samples: {len(df)}")
            else:
                logger.info(f"Sufficient samples: {len(df)}")
            
            if df.shape[1] < 2:
                issues.append(f"Less than 2 features: {df.shape[1]}")
                logger.warning(f"Less than 2 features: {df.shape[1]}")
            else:
                logger.info(f"Sufficient features: {df.shape[1]}")
            
            # Check for infinite values
            inf_count = np.isinf(df.values).sum()
            if inf_count > 0:
                issues.append(f"Found {inf_count} infinite values")
                logger.warning(f"Infinite values: {inf_count}")
                df = df.replace([np.inf, -np.inf], np.nan)
                for col in df.columns:
                    if df[col].isnull().any():
                        df[col].fillna(df[col].median(), inplace=True)
                logger.info("Replaced infinite values with median")
            
            # Check for remaining NaN values
            nan_count = df.isnull().sum().sum()
            if nan_count > 0:
                issues.append(f"Still has {nan_count} NaN values")
                logger.warning(f"NaN values remaining: {nan_count}")
            
            return df, issues
        
        def split_data(df, test_size, random_state):
            # Split data into train and test sets
            logger.info(f"Splitting data (test_size: {test_size})...")
            
            if test_size == 0 or test_size is None:
                logger.info("No test split (all data for training)")
                return df, pd.DataFrame(columns=df.columns)
            
            X_train, X_test = train_test_split(df, test_size=test_size, random_state=random_state)
            logger.info(f"Train: {len(X_train)} samples")
            logger.info(f"Test: {len(X_test)} samples")
            
            return X_train, X_test
        
        def scale_data(X_train, X_test, method):
            # Scale features using specified scaling method
            logger.info(f"Scaling features (method: {method})...")
            
            if method == 'none':
                logger.info("No scaling applied")
                return X_train, X_test, None
            
            if method == 'standard':
                scaler = StandardScaler()
            elif method == 'minmax':
                scaler = MinMaxScaler()
            elif method == 'robust':
                scaler = RobustScaler()
            else:
                raise ValueError(f"Unknown scaling method: {method}")
            
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test) if not X_test.empty else X_test.values
            
            X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)
            X_test_df = pd.DataFrame(X_test_scaled, columns=X_test.columns) if not X_test.empty else X_test
            
            logger.info("Scaling completed")
            return X_train_df, X_test_df, scaler
        
        # Parse command line arguments
        parser = argparse.ArgumentParser(description="Preprocess data for clustering")
        parser.add_argument("--input_data", required=True)
        parser.add_argument("--test_size", type=float, default=0.2)
        parser.add_argument("--random_state", type=int, default=42)
        parser.add_argument("--scaling_method", default='standard')
        parser.add_argument("--missing_method", default='mean')
        parser.add_argument("--categorical_action", default='drop')
        parser.add_argument("--outlier_method", default='clip')
        parser.add_argument("--outlier_threshold", type=float, default=3.0)
        parser.add_argument("--output_train", required=True)
        parser.add_argument("--output_test", required=True)
        parser.add_argument("--output_scaler", required=True)
        parser.add_argument("--output_report", required=True)
        args = parser.parse_args()
        
        logger.info("="*80)
        logger.info("DATA PREPROCESSING COMPONENT")
        logger.info("="*80)
        logger.info(f"Input: {args.input_data}")
        logger.info(f"Test size: {args.test_size}")
        logger.info(f"Scaling: {args.scaling_method}")
        logger.info(f"Missing: {args.missing_method}")
        logger.info(f"Categorical: {args.categorical_action}")
        logger.info(f"Outliers: {args.outlier_method}")
        logger.info("")
        
        try:
            # Ensure output directories exist
            ensure_directory_exists(args.output_train)
            ensure_directory_exists(args.output_test)
            ensure_directory_exists(args.output_scaler)
            ensure_directory_exists(args.output_report)
            
            # Load data
            data_file = find_data_file(args.input_data)
            df = load_data(data_file)
            original_shape = df.shape
            original_columns = list(df.columns)
            
            # Initialize tracking variables
            dropped_cols = []
            label_encoders = {}
            
            # Preprocessing pipeline
            numeric_cols, categorical_cols = analyze_columns(df)
            df = handle_categorical(df, categorical_cols, args.categorical_action, dropped_cols, label_encoders)
            df = handle_missing(df, args.missing_method)
            df = remove_constant_columns(df, dropped_cols)
            df = handle_outliers(df, args.outlier_method, args.outlier_threshold)
            df, issues = check_data_quality(df)
            
            final_columns = list(df.columns)
            
            # Split and scale data
            X_train, X_test = split_data(df, args.test_size, args.random_state)
            X_train_scaled, X_test_scaled, scaler = scale_data(X_train, X_test, args.scaling_method)
            
            # Save train data
            train_dir = os.path.dirname(args.output_train)
            if train_dir:
                os.makedirs(train_dir, exist_ok=True)
            X_train_scaled.to_csv(args.output_train, index=False)
            logger.info(f"Train data saved: {args.output_train}")
            
            # Save test data
            test_dir = os.path.dirname(args.output_test)
            if test_dir:
                os.makedirs(test_dir, exist_ok=True)
            if not X_test_scaled.empty:
                X_test_scaled.to_csv(args.output_test, index=False)
            else:
                pd.DataFrame(columns=X_train_scaled.columns).to_csv(args.output_test, index=False)
            logger.info(f"Test data saved: {args.output_test}")
            
            # Save scaler and encoders
            scaler_dir = os.path.dirname(args.output_scaler)
            if scaler_dir:
                os.makedirs(scaler_dir, exist_ok=True)
            with open(args.output_scaler, 'wb') as f:
                pickle.dump({
                    'scaler': scaler, 
                    'label_encoders': label_encoders, 
                    'final_columns': final_columns
                }, f)
            logger.info(f"Scaler saved: {args.output_scaler}")
            
            # Generate preprocessing report
            report = {
                'original_shape': list(original_shape),
                'final_shape': [len(X_train_scaled) + len(X_test_scaled), X_train_scaled.shape[1]],
                'original_columns': original_columns,
                'final_columns': final_columns,
                'dropped_columns': dropped_cols,
                'categorical_action': args.categorical_action,
                'missing_method': args.missing_method,
                'outlier_method': args.outlier_method,
                'scaling_method': args.scaling_method,
                'test_size': args.test_size,
                'train_samples': len(X_train_scaled),
                'test_samples': len(X_test_scaled),
                'quality_issues': issues
            }
            
            report_dir = os.path.dirname(args.output_report)
            if report_dir:
                os.makedirs(report_dir, exist_ok=True)
            with open(args.output_report, 'w') as f:
                json.dump(report, f, indent=2)
            logger.info(f"Report saved: {args.output_report}")
            
            # Print summary
            logger.info("")
            logger.info("="*80)
            logger.info("PREPROCESSING COMPLETED SUCCESSFULLY")
            logger.info("="*80)
            logger.info(f"Original: {original_shape[0]} rows x {original_shape[1]} cols")
            logger.info(f"Final: {report['final_shape'][0]} rows x {report['final_shape'][1]} cols")
            logger.info(f"Dropped columns: {len(dropped_cols)}")
            logger.info(f"Train samples: {len(X_train_scaled)}")
            logger.info(f"Test samples: {len(X_test_scaled)}")
            if issues:
                logger.info(f"Quality issues: {len(issues)}")
            logger.info("="*80)
            
        except Exception as e:
            logger.error(f"ERROR: {str(e)}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    args:
      - --input_data
      - {inputPath: input_data}
      - --test_size
      - {inputValue: test_size}
      - --random_state
      - {inputValue: random_state}
      - --scaling_method
      - {inputValue: scaling_method}
      - --missing_method
      - {inputValue: missing_method}
      - --categorical_action
      - {inputValue: categorical_action}
      - --outlier_method
      - {inputValue: outlier_method}
      - --outlier_threshold
      - {inputValue: outlier_threshold}
      - --output_train
      - {outputPath: train}
      - --output_test
      - {outputPath: test}
      - --output_scaler
      - {outputPath: scaler}
      - --output_report
      - {outputPath: report}
