name: Data Preprocessing Component
description: Comprehensive preprocessing for clustering with ground truth handling.

inputs:
  - name: input_data
    type: Data
    description: 'Input data from data loading component'
  - name: ground_truth
    type: Data
    description: 'Ground truth labels from data loading (optional)'
    optional: true
  - name: test_size
    type: Float
    description: 'Test split proportion (0-1)'
    default: '0.2'
  - name: random_state
    type: Integer
    description: 'Random seed for reproducibility'
    default: '42'
  - name: scaling_method
    type: String
    description: 'Scaling method: standard, minmax, robust, none'
    default: 'standard'
  - name: missing_method
    type: String
    description: 'Missing value handling: mean, median, drop, zero'
    default: 'mean'
  - name: categorical_action
    type: String
    description: 'Categorical handling: encode, drop, onehot'
    default: 'drop'
  - name: outlier_method
    type: String
    description: 'Outlier handling: clip, remove, none'
    default: 'clip'
  - name: outlier_threshold
    type: Float
    description: 'Z-score threshold for outliers'
    default: '3.0'

outputs:
  - name: train
    type: Data
    description: 'Training dataset after preprocessing'
  - name: test
    type: Data
    description: 'Test dataset after preprocessing'
  - name: scaler
    type: Data
    description: 'Fitted scaler object'
  - name: report
    type: Data
    description: 'Preprocessing report with statistics'
  - name: train_ground_truth
    type: Data
    description: 'Training ground truth labels (if provided)'
  - name: test_ground_truth
    type: Data
    description: 'Test ground truth labels (if provided)'

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        pip install pandas==2.0.3 numpy==1.24.3 scikit-learn==1.3.0 scipy==1.11.1 && exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import json
        import pickle
        import warnings
        import pandas as pd
        import numpy as np
        import logging
        import glob
        from pathlib import Path
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder
        from sklearn.impute import SimpleImputer
        from scipy import stats
        
        warnings.filterwarnings('ignore')
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('preprocessing')
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
        
        def find_data_file(input_path):
            logger.info(f"Searching for data file in: {input_path}")
            
            if os.path.isfile(input_path):
                logger.info(f"Input is a file: {input_path}")
                return input_path
            
            if os.path.isdir(input_path):
                logger.info(f"Input is a directory, searching for data file...")
                patterns = ['*.csv', '*.parquet', '*.pq', '*.npy', 'data.csv', 'data']
                
                for pattern in patterns:
                    files = glob.glob(os.path.join(input_path, pattern))
                    if files:
                        data_file = files[0]
                        logger.info(f"Found data file: {data_file}")
                        return data_file
                
                all_files = [f for f in os.listdir(input_path) 
                            if os.path.isfile(os.path.join(input_path, f)) 
                            and not f.startswith('.')]
                if all_files:
                    data_file = os.path.join(input_path, all_files[0])
                    logger.info(f"Using first file: {data_file}")
                    return data_file
                
                raise FileNotFoundError(f"No data file found in directory: {input_path}")
            
            raise FileNotFoundError(f"Input path not found: {input_path}")
        
        def load_data(input_path):
            logger.info(f"Loading data from: {input_path}")
            
            if input_path.endswith(('.parquet', '.pq')):
                df = pd.read_parquet(input_path)
            else:
                df = pd.read_csv(input_path)
            
            logger.info(f"Loaded: {df.shape[0]} rows x {df.shape[1]} columns")
            return df
        
        def load_ground_truth(ground_truth_path):
            """Load ground truth labels if available"""
            
            if not ground_truth_path or ground_truth_path.lower() == 'none':
                logger.info("No ground truth provided")
                return None
            
            logger.info(f"Loading ground truth from: {ground_truth_path}")
            
            try:
                gt_file = find_data_file(ground_truth_path)
                
                # Check if file is empty
                if os.path.getsize(gt_file) == 0:
                    logger.info("Ground truth file is empty")
                    return None
                
                # Load based on extension
                if gt_file.endswith('.npy'):
                    ground_truth = np.load(gt_file)
                    logger.info(f"Loaded ground truth: {len(ground_truth)} labels")
                    return ground_truth
                elif gt_file.endswith('.csv'):
                    df_gt = pd.read_csv(gt_file)
                    if len(df_gt.columns) == 1:
                        ground_truth = df_gt.iloc[:, 0].values
                    else:
                        ground_truth = df_gt.iloc[:, -1].values
                    logger.info(f"Loaded ground truth: {len(ground_truth)} labels")
                    return ground_truth
                else:
                    logger.warning(f"Unknown ground truth format: {gt_file}")
                    return None
                    
            except FileNotFoundError:
                logger.info("Ground truth file not found")
                return None
            except Exception as e:
                logger.warning(f"Could not load ground truth: {str(e)}")
                return None
        
        def analyze_columns(df):
            logger.info("Analyzing columns...")
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
            logger.info(f"Numeric columns: {len(numeric_cols)}")
            logger.info(f"Categorical columns: {len(categorical_cols)}")
            return numeric_cols, categorical_cols
        
        def handle_categorical(df, categorical_cols, action, dropped_cols, label_encoders):
            if not categorical_cols:
                return df
            
            logger.info(f"Handling categorical columns (action: {action})...")
            df_processed = df.copy()
            
            for col in categorical_cols:
                unique_count = df[col].nunique()
                
                if action == 'drop':
                    df_processed = df_processed.drop(columns=[col])
                    dropped_cols.append(col)
                    logger.info(f"Dropped: {col}")
                    
                elif action == 'encode':
                    try:
                        le = LabelEncoder()
                        df_processed[col] = le.fit_transform(df[col].astype(str))
                        label_encoders[col] = le
                        logger.info(f"Label encoded: {col} ({unique_count} categories)")
                    except Exception as e:
                        logger.warning(f"Could not encode {col}, dropping it: {e}")
                        df_processed = df_processed.drop(columns=[col])
                        dropped_cols.append(col)
                        
                elif action == 'onehot':
                    if unique_count <= 10:
                        dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)
                        df_processed = pd.concat([df_processed.drop(columns=[col]), dummies], axis=1)
                        logger.info(f"One-hot encoded: {col}")
                    else:
                        logger.warning(f"Too many categories in {col} ({unique_count}), dropping it")
                        df_processed = df_processed.drop(columns=[col])
                        dropped_cols.append(col)
            
            return df_processed
        
        def handle_missing(df, method):
            logger.info(f"Handling missing values (method: {method})...")
            missing_before = df.isnull().sum().sum()
            logger.info(f"Missing values: {missing_before}")
            
            if missing_before == 0:
                logger.info("No missing values")
                return df
            
            if method == 'drop':
                df_cleaned = df.dropna()
                logger.info(f"Dropped {len(df) - len(df_cleaned)} rows")
            elif method == 'zero':
                df_cleaned = df.fillna(0)
                logger.info("Filled with 0")
            elif method in ['mean', 'median']:
                imputer = SimpleImputer(strategy=method)
                df_cleaned = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)
                logger.info(f"Imputed with {method}")
            else:
                raise ValueError(f"Unknown method: {method}")
            
            return df_cleaned
        
        def remove_constant_columns(df, dropped_cols):
            logger.info("Checking for constant columns...")
            constant_cols = [col for col in df.columns if df[col].nunique() <= 1]
            
            if constant_cols:
                df_filtered = df.drop(columns=constant_cols)
                dropped_cols.extend(constant_cols)
                logger.info(f"Removed {len(constant_cols)} constant columns: {constant_cols}")
                return df_filtered
            else:
                logger.info("No constant columns")
                return df
        
        def handle_outliers(df, method, threshold):
            logger.info(f"Handling outliers (method: {method}, threshold: {threshold})...")
            
            if method == 'none':
                logger.info("Skipping outlier handling")
                return df, []
            
            df_processed = df.copy()
            outlier_counts = {}
            removed_indices = []
            
            for col in df.columns:
                try:
                    z_scores = np.abs(stats.zscore(df_processed[col]))
                    outliers = (z_scores > threshold).sum()
                    
                    if outliers > 0:
                        outlier_counts[col] = outliers
                        if method == 'clip':
                            mean = df_processed[col].mean()
                            std = df_processed[col].std()
                            lower = mean - threshold * std
                            upper = mean + threshold * std
                            df_processed[col] = df_processed[col].clip(lower, upper)
                            logger.info(f"Column {col}: clipped {outliers} outliers")
                except Exception as e:
                    logger.warning(f"Could not process outliers for {col}: {e}")
            
            if method == 'remove':
                initial_len = len(df_processed)
                z_scores = np.abs(stats.zscore(df_processed))
                mask = (z_scores < threshold).all(axis=1)
                removed_indices = np.where(~mask)[0].tolist()
                df_processed = df_processed[mask]
                removed = initial_len - len(df_processed)
                logger.info(f"Removed {removed} rows with outliers")
                logger.info(f"Removed indices: {len(removed_indices)}")
            
            return df_processed, removed_indices
        
        def check_data_quality(df):
            logger.info("Data quality check...")
            issues = []
            
            if len(df) < 10:
                issues.append(f"Very few samples: {len(df)}")
                logger.warning(f"Very few samples: {len(df)}")
            else:
                logger.info(f"Sufficient samples: {len(df)}")
            
            if df.shape[1] < 2:
                issues.append(f"Less than 2 features: {df.shape[1]}")
                logger.warning(f"Less than 2 features: {df.shape[1]}")
            else:
                logger.info(f"Sufficient features: {df.shape[1]}")
            
            inf_count = np.isinf(df.values).sum()
            if inf_count > 0:
                issues.append(f"Found {inf_count} infinite values")
                logger.warning(f"Infinite values: {inf_count}")
                df = df.replace([np.inf, -np.inf], np.nan)
                for col in df.columns:
                    if df[col].isnull().any():
                        df[col].fillna(df[col].median(), inplace=True)
                logger.info("Replaced infinite values with median")
            
            nan_count = df.isnull().sum().sum()
            if nan_count > 0:
                issues.append(f"Still has {nan_count} NaN values")
                logger.warning(f"NaN values remaining: {nan_count}")
            
            return df, issues
        
        def split_data(df, ground_truth, test_size, random_state):
            """Split data and ground truth together"""
            logger.info(f"Splitting data (test_size: {test_size})...")
            
            if test_size == 0 or test_size is None:
                logger.info("No test split (all data for training)")
                empty_df = pd.DataFrame(columns=df.columns)
                
                if ground_truth is not None:
                    return df, empty_df, ground_truth, np.array([])
                else:
                    return df, empty_df, None, None
            
            if ground_truth is not None:
                # Stratified split if ground truth available
                logger.info("Performing stratified split using ground truth")
                X_train, X_test, y_train, y_test = train_test_split(
                    df, ground_truth, 
                    test_size=test_size, 
                    random_state=random_state,
                    stratify=ground_truth
                )
                logger.info(f"Train: {len(X_train)} samples")
                logger.info(f"Test: {len(X_test)} samples")
                logger.info(f"Train GT: {len(y_train)} labels")
                logger.info(f"Test GT: {len(y_test)} labels")
                
                return X_train, X_test, y_train, y_test
            else:
                # Regular split without ground truth
                logger.info("Performing regular split (no ground truth)")
                X_train, X_test = train_test_split(
                    df, test_size=test_size, random_state=random_state
                )
                logger.info(f"Train: {len(X_train)} samples")
                logger.info(f"Test: {len(X_test)} samples")
                
                return X_train, X_test, None, None
        
        def scale_data(X_train, X_test, method):
            logger.info(f"Scaling features (method: {method})...")
            
            if method == 'none':
                logger.info("No scaling applied")
                return X_train, X_test, None
            
            if method == 'standard':
                scaler = StandardScaler()
            elif method == 'minmax':
                scaler = MinMaxScaler()
            elif method == 'robust':
                scaler = RobustScaler()
            else:
                raise ValueError(f"Unknown scaling method: {method}")
            
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test) if not X_test.empty else X_test.values
            
            X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)
            X_test_df = pd.DataFrame(X_test_scaled, columns=X_test.columns) if not X_test.empty else X_test
            
            logger.info("Scaling completed")
            return X_train_df, X_test_df, scaler
        
        parser = argparse.ArgumentParser(description="Preprocess data for clustering")
        parser.add_argument("--input_data", required=True)
        parser.add_argument("--ground_truth", default=None)
        parser.add_argument("--test_size", type=float, default=0.2)
        parser.add_argument("--random_state", type=int, default=42)
        parser.add_argument("--scaling_method", default='standard')
        parser.add_argument("--missing_method", default='mean')
        parser.add_argument("--categorical_action", default='drop')
        parser.add_argument("--outlier_method", default='clip')
        parser.add_argument("--outlier_threshold", type=float, default=3.0)
        parser.add_argument("--output_train", required=True)
        parser.add_argument("--output_test", required=True)
        parser.add_argument("--output_scaler", required=True)
        parser.add_argument("--output_report", required=True)
        parser.add_argument("--output_train_ground_truth", required=True)
        parser.add_argument("--output_test_ground_truth", required=True)
        args = parser.parse_args()
        
        logger.info("="*80)
        logger.info("DATA PREPROCESSING COMPONENT (WITH GROUND TRUTH SUPPORT)")
        logger.info("="*80)
        logger.info(f"Input: {args.input_data}")
        logger.info(f"Ground truth: {args.ground_truth if args.ground_truth else 'None'}")
        logger.info(f"Test size: {args.test_size}")
        logger.info("")
        
        try:
            # Ensure output directories exist
            ensure_directory_exists(args.output_train)
            ensure_directory_exists(args.output_test)
            ensure_directory_exists(args.output_scaler)
            ensure_directory_exists(args.output_report)
            ensure_directory_exists(args.output_train_ground_truth)
            ensure_directory_exists(args.output_test_ground_truth)
            
            # Load data and ground truth
            data_file = find_data_file(args.input_data)
            df = load_data(data_file)
            ground_truth = load_ground_truth(args.ground_truth)
            
            # Validate ground truth length
            if ground_truth is not None and len(ground_truth) != len(df):
                raise ValueError(
                    f"Ground truth length mismatch: data has {len(df)} samples, "
                    f"ground truth has {len(ground_truth)} labels"
                )
            
            original_shape = df.shape
            original_columns = list(df.columns)
            original_indices = np.arange(len(df))
            
            # Initialize tracking variables
            dropped_cols = []
            label_encoders = {}
            
            # Preprocessing pipeline
            numeric_cols, categorical_cols = analyze_columns(df)
            df = handle_categorical(df, categorical_cols, args.categorical_action, dropped_cols, label_encoders)
            df = handle_missing(df, args.missing_method)
            df = remove_constant_columns(df, dropped_cols)
            df, removed_indices = handle_outliers(df, args.outlier_method, args.outlier_threshold)
            
            # Update ground truth if rows were removed
            if ground_truth is not None and removed_indices:
                logger.info(f"Removing {len(removed_indices)} ground truth labels for removed rows")
                mask = np.ones(len(ground_truth), dtype=bool)
                mask[removed_indices] = False
                ground_truth = ground_truth[mask]
                logger.info(f"Ground truth now has {len(ground_truth)} labels")
            
            df, issues = check_data_quality(df)
            final_columns = list(df.columns)
            
            # Split data (with ground truth if available)
            X_train, X_test, y_train, y_test = split_data(
                df, ground_truth, args.test_size, args.random_state
            )
            
            # Scale data
            X_train_scaled, X_test_scaled, scaler = scale_data(
                X_train, X_test, args.scaling_method
            )
            
            # Save train data
            X_train_scaled.to_csv(args.output_train, index=False)
            logger.info(f"Train data saved: {args.output_train}")
            
            # Save test data
            if not X_test_scaled.empty:
                X_test_scaled.to_csv(args.output_test, index=False)
            else:
                pd.DataFrame(columns=X_train_scaled.columns).to_csv(args.output_test, index=False)
            logger.info(f"Test data saved: {args.output_test}")
            
            # Save scaler and encoders
            with open(args.output_scaler, 'wb') as f:
                pickle.dump({
                    'scaler': scaler, 
                    'label_encoders': label_encoders, 
                    'final_columns': final_columns
                }, f)
            logger.info(f"Scaler saved: {args.output_scaler}")
            
            # Save ground truth splits
            if y_train is not None:
                np.save(args.output_train_ground_truth, y_train)
                logger.info(f"Train ground truth saved: {args.output_train_ground_truth}")
                logger.info(f"Train GT shape: {y_train.shape}")
            else:
                with open(args.output_train_ground_truth, 'w') as f:
                    f.write("")
                logger.info("No train ground truth (empty file created)")
            
            if y_test is not None:
                np.save(args.output_test_ground_truth, y_test)
                logger.info(f"Test ground truth saved: {args.output_test_ground_truth}")
                logger.info(f"Test GT shape: {y_test.shape}")
            else:
                with open(args.output_test_ground_truth, 'w') as f:
                    f.write("")
                logger.info("No test ground truth (empty file created)")
            
            # Generate preprocessing report
            report = {
                'original_shape': list(original_shape),
                'final_shape': [len(X_train_scaled) + len(X_test_scaled), X_train_scaled.shape[1]],
                'original_columns': original_columns,
                'final_columns': final_columns,
                'dropped_columns': dropped_cols,
                'categorical_action': args.categorical_action,
                'missing_method': args.missing_method,
                'outlier_method': args.outlier_method,
                'scaling_method': args.scaling_method,
                'test_size': args.test_size,
                'train_samples': len(X_train_scaled),
                'test_samples': len(X_test_scaled),
                'quality_issues': issues,
                'has_ground_truth': ground_truth is not None,
                'ground_truth_info': None
            }
            
            if ground_truth is not None:
                report['ground_truth_info'] = {
                    'total_labels': int(len(ground_truth)),
                    'train_labels': int(len(y_train)) if y_train is not None else 0,
                    'test_labels': int(len(y_test)) if y_test is not None else 0,
                    'unique_classes': int(len(np.unique(ground_truth))),
                    'class_distribution': {
                        int(label): int(count) 
                        for label, count in zip(*np.unique(ground_truth, return_counts=True))
                    }
                }
            
            with open(args.output_report, 'w') as f:
                json.dump(report, f, indent=2)
            logger.info(f"Report saved: {args.output_report}")
            
            # Print summary
            logger.info("")
            logger.info("="*80)
            logger.info("PREPROCESSING COMPLETED SUCCESSFULLY")
            logger.info("="*80)
            logger.info(f"Original: {original_shape[0]} rows x {original_shape[1]} cols")
            logger.info(f"Final: {report['final_shape'][0]} rows x {report['final_shape'][1]} cols")
            logger.info(f"Train samples: {len(X_train_scaled)}")
            logger.info(f"Test samples: {len(X_test_scaled)}")
            if ground_truth is not None:
                logger.info(f"Ground truth: {report['ground_truth_info']['unique_classes']} classes")
            logger.info("="*80)
            
        except Exception as e:
            logger.error(f"ERROR: {str(e)}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    args:
      - --input_data
      - {inputPath: input_data}
      - --ground_truth
      - {inputPath: ground_truth}
      - --test_size
      - {inputValue: test_size}
      - --random_state
      - {inputValue: random_state}
      - --scaling_method
      - {inputValue: scaling_method}
      - --missing_method
      - {inputValue: missing_method}
      - --categorical_action
      - {inputValue: categorical_action}
      - --outlier_method
      - {inputValue: outlier_method}
      - --outlier_threshold
      - {inputValue: outlier_threshold}
      - --output_train
      - {outputPath: train}
      - --output_test
      - {outputPath: test}
      - --output_scaler
      - {outputPath: scaler}
      - --output_report
      - {outputPath: report}
      - --output_train_ground_truth
      - {outputPath: train_ground_truth}
      - --output_test_ground_truth
      - {outputPath: test_ground_truth}
