name: Data Loading Component
description: Load and validate data for clustering pipelines. Accepts either a file path string or a Dataset directory from previous component. Supports CSV, Parquet, Excel, and JSON formats. Performs comprehensive data validation and generates metadata.

inputs:
  - name: input_dataset
    type: Dataset
    description: 'Input dataset from previous component (directory containing data file)'
  - name: file_format
    type: String
    description: 'File format (auto, csv, parquet, excel, json)'
    default: 'auto'
  - name: encoding
    type: String
    description: 'File encoding for CSV files'
    default: 'utf-8'
  - name: sep
    type: String
    description: 'CSV separator character'
    default: ','

outputs:
  - name: output_data
    type: Data
  - name: output_metadata
    type: Data

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        pip install pandas==2.0.3 numpy==1.24.3 pyarrow==12.0.1 openpyxl==3.1.2 && exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import json
        import pandas as pd
        import numpy as np
        from pathlib import Path
        import logging
        import glob
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('data_loading')
        
        def ensure_directory_exists(file_path):
            directory = os.path.dirname(file_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
        
        def find_data_file(input_path):
            # Find the actual data file in the input path.
            # Input can be either a file or a directory.
            logger.info(f"Searching for data file in: {input_path}")
            
            # If input is a file, return it directly
            if os.path.isfile(input_path):
                logger.info(f"Input is a file: {input_path}")
                return input_path
            
            # If input is a directory, find the data file inside
            if os.path.isdir(input_path):
                logger.info(f"Input is a directory, searching for data file...")
                
                # Search for common data file extensions
                patterns = ['*.csv', '*.parquet', '*.pq', '*.xlsx', '*.xls', '*.json']
                
                for pattern in patterns:
                    files = glob.glob(os.path.join(input_path, pattern))
                    if files:
                        data_file = files[0]  # Take the first match
                        logger.info(f"Found data file: {data_file}")
                        return data_file
                
                # If no data file found with extensions, list all files
                all_files = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]
                if all_files:
                    data_file = os.path.join(input_path, all_files[0])
                    logger.info(f"No standard extension found, using first file: {data_file}")
                    return data_file
                
                raise FileNotFoundError(f"No data file found in directory: {input_path}")
            
            raise FileNotFoundError(f"Input path not found: {input_path}")
        
        def load_data(input_path, file_format='auto', encoding='utf-8', sep=','):
            logger.info(f"Loading data from: {input_path}")
            
            # Auto-detect format
            if file_format == 'auto':
                ext = Path(input_path).suffix.lower()
                format_map = {
                    '.csv': 'csv',
                    '.parquet': 'parquet',
                    '.pq': 'parquet',
                    '.xlsx': 'excel',
                    '.xls': 'excel',
                    '.json': 'json'
                }
                file_format = format_map.get(ext, 'csv')
            
            logger.info(f"Format: {file_format}")
            
            # Load based on format
            if file_format == 'csv':
                df = pd.read_csv(input_path, encoding=encoding, sep=sep)
            elif file_format == 'parquet':
                df = pd.read_parquet(input_path)
            elif file_format == 'excel':
                df = pd.read_excel(input_path)
            elif file_format == 'json':
                df = pd.read_json(input_path)
            else:
                raise ValueError(f"Unsupported format: {file_format}")
            
            logger.info(f"Loaded successfully: {df.shape[0]} rows x {df.shape[1]} columns")
            return df
        
        def validate_data(df):
            logger.info("Validating data...")
            
            validation = {
                'is_valid': True,
                'errors': [],
                'warnings': []
            }
            
            # Check 1: Empty dataset
            if df.empty:
                validation['is_valid'] = False
                validation['errors'].append("Dataset is empty")
                return validation, {}
            
            logger.info(f"Dataset has {len(df)} samples")
            
            # Check 2: Minimum samples
            if len(df) < 2:
                validation['is_valid'] = False
                validation['errors'].append("Need at least 2 samples")
                return validation, {}
            
            # Check 3: Duplicate columns
            if df.columns.duplicated().any():
                validation['warnings'].append("Duplicate column names")
                logger.warning("Duplicate column names detected")
            
            # Identify numeric and categorical columns
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
            
            logger.info(f"Numeric columns: {len(numeric_cols)}")
            logger.info(f"Categorical columns: {len(categorical_cols)}")
            
            if len(numeric_cols) == 0:
                validation['warnings'].append("No numeric columns found")
                logger.warning("No numeric columns for clustering")
            
            # Check 4: Missing values
            missing_total = df.isnull().sum().sum()
            missing_pct = (missing_total / (df.shape[0] * df.shape[1])) * 100
            
            missing_by_col = {}
            if missing_total > 0:
                msg = f"Found {missing_total} missing values ({missing_pct:.2f}%)"
                validation['warnings'].append(msg)
                logger.warning(msg)
                for col in df.columns:
                    miss_count = df[col].isnull().sum()
                    if miss_count > 0:
                        missing_by_col[col] = int(miss_count)
                        logger.info(f"  - {col}: {miss_count} ({miss_count/len(df)*100:.1f}%)")
            else:
                logger.info("No missing values")
            
            # Check 5: Infinite values
            inf_count = 0
            for col in numeric_cols:
                col_inf = df[col].isin([np.inf, -np.inf]).sum()
                inf_count += col_inf
            
            if inf_count > 0:
                msg = f"Found {inf_count} infinite values"
                validation['warnings'].append(msg)
                logger.warning(msg)
            else:
                logger.info("No infinite values")
            
            # Check 6: Constant columns
            constant_cols = [col for col in df.columns if df[col].nunique() <= 1]
            if constant_cols:
                msg = f"Constant columns: {constant_cols}"
                validation['warnings'].append(msg)
                logger.warning(msg)
            else:
                logger.info("No constant columns")
            
            # Build metadata
            metadata = {
                'n_samples': int(df.shape[0]),
                'n_features': int(df.shape[1]),
                'columns': list(df.columns),
                'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()},
                'numeric_columns': numeric_cols,
                'categorical_columns': categorical_cols,
                'missing_values': {
                    'total': int(missing_total),
                    'percentage': float(missing_pct),
                    'by_column': missing_by_col
                },
                'constant_columns': constant_cols,
                'memory_mb': float(df.memory_usage(deep=True).sum() / 1024**2)
            }
            
            # Add statistics for numeric columns
            if numeric_cols:
                metadata['statistics'] = {}
                for col in numeric_cols:
                    try:
                        metadata['statistics'][col] = {
                            'mean': float(df[col].mean()),
                            'std': float(df[col].std()),
                            'min': float(df[col].min()),
                            'max': float(df[col].max()),
                            'median': float(df[col].median()),
                            'q25': float(df[col].quantile(0.25)),
                            'q75': float(df[col].quantile(0.75))
                        }
                    except:
                        pass
            
            metadata['validation'] = validation
            return validation, metadata
        
        parser = argparse.ArgumentParser(description="Load and validate data for clustering")
        parser.add_argument("--input_dataset", required=True)
        parser.add_argument("--file_format", default='auto')
        parser.add_argument("--encoding", default='utf-8')
        parser.add_argument("--sep", default=',')
        parser.add_argument("--output_data", required=True)
        parser.add_argument("--output_metadata", required=True)
        args = parser.parse_args()
        
        logger.info("="*80)
        logger.info("DATA LOADING COMPONENT")
        logger.info("="*80)
        logger.info(f"Input dataset: {args.input_dataset}")
        logger.info(f"Format: {args.file_format}")
        logger.info(f"Encoding: {args.encoding}")
        logger.info(f"Separator: {args.sep}")
        logger.info("")
        
        try:
            # Ensure output directories exist
            ensure_directory_exists(args.output_data)
            ensure_directory_exists(args.output_metadata)
            
            # Find the actual data file in the input dataset
            data_file_path = find_data_file(args.input_dataset)
            
            # Load data
            df = load_data(data_file_path, args.file_format, args.encoding, args.sep)
            
            # Validate data
            validation, metadata = validate_data(df)
            
            # Check if valid
            if not validation['is_valid']:
                logger.error("VALIDATION FAILED")
                for error in validation['errors']:
                    logger.error(f"Error: {error}")
                sys.exit(1)
            
            # Show warnings
            if validation['warnings']:
                logger.warning(f"Warnings: {len(validation['warnings'])}")
                for warning in validation['warnings']:
                    logger.warning(f"  - {warning}")
            
            # Save data
            df.to_csv(args.output_data, index=False)
            logger.info(f"Data saved: {args.output_data}")
            
            # Save metadata
            with open(args.output_metadata, 'w') as f:
                json.dump(metadata, f, indent=2)
            logger.info(f"Metadata saved: {args.output_metadata}")
            
            logger.info("")
            logger.info("="*80)
            logger.info("DATA LOADING COMPLETED SUCCESSFULLY")
            logger.info("="*80)
            logger.info(f"Samples: {metadata['n_samples']}")
            logger.info(f"Features: {metadata['n_features']}")
            logger.info(f"  - Numeric: {len(metadata['numeric_columns'])}")
            logger.info(f"  - Categorical: {len(metadata['categorical_columns'])}")
            logger.info(f"Missing values: {metadata['missing_values']['percentage']:.2f}%")
            logger.info(f"Memory: {metadata['memory_mb']:.2f} MB")
            logger.info("="*80)
            
        except FileNotFoundError as e:
            logger.error(f"ERROR: {str(e)}")
            sys.exit(1)
        except Exception as e:
            logger.error(f"ERROR: {str(e)}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    args:
      - --input_dataset
      - {inputPath: input_dataset}
      - --file_format
      - {inputValue: file_format}
      - --encoding
      - {inputValue: encoding}
      - --sep
      - {inputValue: sep}
      - --output_data
      - {outputPath: output_data}
      - --output_metadata
      - {outputPath: output_metadata}
